\chapter{Simulations}
\label{C:simulations}
This chapter is a computational companion to chapter~\ref{C:stein-proof}.

\section{Preliminaries}
First, we provide simulations accompanying
section~\ref{S:stein-proof-preliminaries}.  We generate i.i.d. samples
$\{u_i\}_{i=1}^{N} \sim \Normal(-1, 1)$ and
$\{u_i\}_{i=N+1}^{2N} \sim \Normal(1, 1)$ for exponentially-spaced values of
$N \in \{\text{floor}(10^{.5+.5i})\}_{i=1}^7$.  The $u_i$ are scaled and
centered, and for each $N$, we perform $10,000$ permutations.

We plot Monte Carlo estimates of the means of each term, scaled by the
rate of our bound, along with 95\%ile bootstrap confidence intervals
for different values of $p \in \{2, 4, 6, 8\}$.

\begin{figure}[!ht]
  \centering
  \input{./simulations/img/sim1.tex}
  \caption{Log-log plots of values scaled by proven upper bounds of rates, faceted on $p$.}
\end{figure}
Due to the flatness of the curves, we conclude that the bounds we have
proved are of the correct rate.  In addition, we can observe the
behavior of the constants as functions of $p$.  For instance, our
$f_{c_3}(p)$ constant for $\E h_{\Pi}^p$ appears to be an exponential
function of $p$.
\clearpage

Here, to compute the corresponding ``prime'' random variables in the
coupled pair, in each permutation we pick a transposition uniformly at
random among transpositions that switch groups.

\begin{figure}[!ht]
  \centering
  \input{./simulations/img/sim2.tex}
  \caption{Log-log plots of values scaled by proven upper bounds of rates, faceted on $p$.}
\end{figure}
It is possible that the bound of rate $N^{-p/2}$ on
$\E \left [ \left ( \frac{q_{\Pi}'}{d_{\Pi}d_{\Pi}'} \right )^p \right ]$ is a bit conservative.
\clearpage

\section{Approximate Regression Condition}
From the approximate regression condition
\begin{equation*}
  \E [T'_{\Pi} - T_{\Pi} | T_{\Pi}] = -\lambda (T_{\Pi} - R_{\Pi})
\end{equation*}
we get
\begin{equation*}
  \E [T'_{\Pi}| T_{\Pi}] = (1-\lambda) T_{\Pi} - \lambda R_{\Pi}.
\end{equation*}
That is, the conditional expectation of $T'_{\Pi}$ on $T_{\Pi}$ is expected to lie near the line
$(1-\lambda) T_{\Pi}$ with a small perturbation of order $1 / N$ (recall that $\lambda = 2 / N$).

For various values of $N$, we compute 20 permutations that correspond with 20 values of $T_{\Pi}$.
For each $T_{\Pi}$, we draw a transposition $(I, J)$ uniformly at random from the space of our
allowable transpositions, repeating this 100 times.
\begin{figure}[!ht]
  \centering
  %lualatex thesis
  \resizebox{12.0cm}{!}{\input{./simulations/img/sim7.tex}}
  %\includegraphics[scale = .75]{./simulations/sim7.png}
  \caption{Faceted on per-group sample size, $N$.}
\end{figure}

The approximate regression condition appears to hold visually.
\clearpage

\section{Main Bounds}
Here we simulate the main bounds under the same setting as the previous section.
\subsection{Failure of Monte Carlo}
Again, we simulate the conditional expectations of the form $\E[f(T'_{\Pi}, T_{\Pi}) | T_{\Pi}]$
with 1,000 draws from the uniform distribution on all group-switching transpositions $(I, J)$.

\begin{figure}[!ht]
  \centering
  \input{./simulations/img/sim7.tex}
  \caption{Log-log plot of values for each term in the bound,
    simulating the conditional by Monte Carlo.}
\end{figure}
The MC error is too large, and we see some scaled bounds actually increase.
\clearpage

\subsection{Exact Conditional Expectation Calculations}
Here we plot $T'$ for all $N^2$ group-switching transpositions $(I, J)$:
\begin{figure}[!ht]
  \centering
  \input{./simulations/img/sim3.tex}
  \caption{Log-log plot of values for each term in the bound, calculating the conditional
    expectation exactly ($10N$ permutations each).}
\end{figure}

Our bounds appear to be of the correct order or slightly conservative in some cases.  The bounds on
the remainder terms ($\E |R_{\Pi}|$ and $\E |T_{\Pi}R_{\Pi}|$) are of order $N^{1/2}$, but the true
rates are probably lower.
\clearpage

\subsection{Better Rate}
\begin{figure}[!ht]
  \centering
  \input{./simulations/img/sim6.tex}
  \caption{Log-log plot of values for each term in the bound, calculating the conditional
    expectation exactly ($10N$ permutations each).}
\end{figure}


\section{True Rate}
To assess the true rate of convergence, we consider two settings: the earlier Normal setting and
group draws from a Cauchy distribution with location parameters $-1$ and $1$ depending on the
group.  Our bounds include a dependence on $u_{\Delta} = \max_i u_{i} - \min_i u_{i}$.  To better
understand the differences between these two models, we simulate the minimum and maximum scaled
(mean $0$ and sum of squares $2N$) values:
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/sim5.png}
\end{figure}

For $N=1000$, the Normal model typically has $u_{\Delta}$ values around 6.  In contrast, the Cauchy
model has $u_{\Delta}$ values closer to 40.
\clearpage

Here, we plot the empirical Kolmogorov-Smirnov test statistic in the following three settings:
\begin{enumerate}
\item a standard Normal draw of size $N$ (repeated N times to get the empirical distribution)
\item the permutation $t$-statistic under Cauchy sampling (N permutations)
\item the permutation $t$-statistic under Normal sampling (N permutations)
\end{enumerate}
We also add the sum of the five unscaled, simulated bound terms ($200,000$ permutations) from the
previous section.

\begin{figure}[!ht]
  \centering
  \input{./simulations/sim4.tex}
  \caption{Solid black line: $N^{-1/4}$; dashed black line: $N^{-1/2}$}
\end{figure}

It's not a fair comparison to place the sum of the bounds on the same plot because that was computed
over $200,000$ separate permutations instead of the $500$ shared by the other three.  Still, we can
draw some general conclusions.  The normal and two permutation-$t$ K-S statistics decay perfectly at
a rate of $N^{-1/2}$, and our bound follows a rate of $N^{-1/4}$, suggesting that the true rate of
convergence is the former.  Also, the error-bars seem to be increasing in size but are actually
roughly constant due to the log-log scale.

Chen et al. \cite{chen2010normal} provide a simple example
(pp.154-155) in which the sum of i.i.d. random variables yields
\begin{equation*}
  E|W' - W|^3 = \frac{4}{N^{3/2}}
\end{equation*}
with $\lambda = N^{-1}$.  This leads to an $O(N^{-1/4})$ bound, which is suboptimal and apparently
not uncommon when applying this kind of theorem.

\section{Efficient Updates}
Instead of conditioning on the value of $T_{\Pi}$, we condition on the
observed permutation $\pi$.  For $N$ observations in each group, there
are $N^2 \: T_{\Pi}'$ values that come from swapping one value in the first
group with one value in the second.  $T_{\Pi}'$ should not differ much from
$T_{\Pi}$, and calculating the $t$-statistics from scratch is inefficient.

We use an efficient $t$-statistic update rule to easily calculate
millions of $t$-statistics.  The two sample $t$-statistic is given by
\begin{equation*}
  T_{\Pi} = \frac{\bar{x}-\bar{u}}
  {\sqrt{\frac{2}{n}}\sqrt{\frac{1}{2}(S_X^2+S_U^2)}},
\end{equation*}
where $S_X^2=\frac{1}{N-1}(\sum_{i=1}^Nx_i^2-n\bar{x}^2)$.

Let $T_{x_i,u_j}$ be the result of $T'$ by swapping $x_i$ with $u_j$:
\begin{align*}
  \Delta &\equiv u_j-x_i \\
  \bar{x}_{x_i,u_j} &= \bar{x}-\frac{1}{N}x_i+\frac{1}{N}u_j =
  \bar{x}+\frac{\Delta}{N} \\
  \bar{u}_{x_i,u_j} &= \bar{u}+\frac{1}{N}x_i-\frac{1}{N}u_j =
  \bar{u}-\frac{\Delta}{N} \\
  S_{X_{x_i,u_j}}^2 &= \frac{1}{N-1}(\sum_{k=1}^N x_k^2 - x_i^2 +
  u_j^2) - \frac{N}{N-1}\bar{x}^2_{x_i,u_j} \\
  S_{U_{x_i,u_j}}^2 &= \frac{1}{N-1}(\sum_{k=1}^N u_k^2 + x_i^2 -
  u_j^2) - \frac{N}{N-1}\bar{u}^2_{x_i,u_j} \\
  \bar{x}_{x_i,u_j}^2 &= \bar{x}^2 + \frac{2\Delta}{N}\bar{x} +
  \frac{\Delta^2}{N} \\
  \bar{u}_{x_i,u_j}^2 &= \bar{u}^2 - \frac{2\Delta}{N}\bar{u} + \frac{\Delta^2}{N}
\end{align*}

Then
\begin{align*}
  T_{x_i,u_j} &= \frac{\bar{x}_{x_i,u_j}-\bar{u}_{x_i,u_j}}
  {\sqrt{\frac{2}{N}}\sqrt{\frac{1}{2}(S_{X_{x_i,u_j}}^2+S_{U_{x_i,u_j}}^2)}}\\
  &= \frac{\bar{x}-\bar{u}+\frac{2\Delta}{N}}
  {\sqrt{\frac{2}{N}}\sqrt{\frac{1}{2(N-1)}[\sum_{k=1}^N (x_k^2+u_k^2)
      -N(\bar{x}^2+\bar{u}^2+\Delta(\frac{2\bar{x}}{n}
      -\frac{2\bar{u}}{n})+\frac{2}{n^2}\Delta^2)]}}.
\end{align*}
Only the terms involving $\Delta$ need to be recomputed for each of the $N^2$ swaps.

Consider a na\"{\i}ve implementation based on a double for-loop and recomputing each $t$-statistic
anew versus a vectorized approach using the update formula:
\begin{verbatim}
computeAllCond2 <- function(T, N, u, l, x, y){
    minus <- which(l == -1)
    plus <- which(l == 1)
    Tprime <- 1:(N^2)
    for(j in 1:N){
      for(k in 1:N){
        swap <- c(minus[j], plus[k])
        l[swap] <- l[rev(swap)]
        Tprime[N*(j-1)+k] <- t.test(u[l==1], u[l==-1], var.equal=TRUE)$statistic
        l[swap] <- l[rev(swap)]
      }
    }
    data.frame("T" = T, "Tprime" = Tprime, "N" = N, "lambda" = 2 / N)
}

computeAllCond <- function(T, N, u, l, x, y){
  del <- rep(y, length(x)) - rep(x, each = length(y))
  xbar <- mean(x)
  ybar <- mean(y)
  Tprime <- -(xbar - ybar + 2/N*del) /
    (sqrt(2/N)*sqrt(sum(u^2)/(2*(N-1)) - 1/2*N/(N-1)*(xbar^2 + ybar^2 + 2*del/N*(xbar-ybar) + 2*del^2/N^2)))
  data.frame("T" = T, "Tprime" = Tprime, "N" = N, "lambda" = 2 / N)
}
\end{verbatim}

We observe roughly a 2,000 times increase in speed on a problem instance of size $N=100$.  With
byte-compilation and additional tuning, a four order of magnitude increase is possible.
\begin{verbatim}
> system.time(computeAllCond2(T, N, u, l, x, y))
   user  system elapsed
  7.333   0.000   7.334
> system.time(computeAllCond(T, N, u, l, x, y))
   user  system elapsed
  0.005   0.000   0.004
> sum((sort(computeAllCond(T, N, u, l, x, y)$Tprime) - sort(computeAllCond2(T, N, u, l, x, y)$Tprime))^2)
[1] 3.137579e-27
dat <- ldply(rep(floor(10^(seq(1, 3.5, by=.5))), each = 8),
simulateBounds, .parallel = TRUE, .progress = "text")
> print(object.size(dat), units = "Gb")
2.6 Gb
\end{verbatim}

\section{A Different Exchangeable Pair}
Rather than only consider transpositions that swap one element of the
first group with one from the second group, we have a few different
choices.  Let's take the other extreme, where we consider all $(2N)^2$
transpositions, including null transpositions.  There are $N^2$
transpositions within each group, for a total of $2N^2$.  Each of
these does not change the $t$-statistic.  We previously only
considered the $N^2$ transpositions where $I < J$.  There are another
$N^2$ with $I > J$.  These transpositions have exactly the same effect
as the previous group $(I, J) = (J, I)$, and all within-group
transpositions have no effect.

The only changes should be to adjust the weights when taking
conditional expectations (the weights should be $1/2$) and to divide
$\lambda$ by 2.  The new $\lambda$ is $N^{-1}$.

However, every term involving the conditional expectation also has a
division by lambda, so any decrease in the c.e. is cancelled out by a
corresponding decrease in $\lambda$, so there is no change in any of
the simulations.

It's nice that the calculations are invariant to change in the
exchangeable pair.  Whether that holds true for more drastic changes
(e.g. swapping more than 2 elements) is not known.

\section{Generalizations (Null Distribution)}
It is natural to consider generalizations from the univariate data, linear
kernel setting.  We explore whether the randomization distribution is
still Normal with multivariate data and/or a non-linear kernel.
Should the null distribution be non-Normal, we further attempt to determine
whether the approximate regression condition holds.

We generate 100 observations with dimensionality 1, 10, 100, and
1000.  For each set of data, we permute 100 times and plot the
Friedman statistic ($t$-statistic on SVM fitted values) with a linear
kernel against the standard Normal quantiles.  Note that we take the
sign of the Friedman statistic to be positive or negative with equal
probability for ease of comparing distributions.
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/ARC/multivar_qq.png}
\end{figure}

At a sample size of 100 and with univariate data, the standard Normal
is a close fit.  With increasing dimensionality, the Friedman
statistic gets more and more extreme.  One possible explanation is
that it becomes easier to separate two sets of points as the
dimensionality increases.
\clearpage

Here we look at univariate data but with an inhomogeneous kernel
($k(x, x') = (<x,x'>+1)^d$) of degree $d$:
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/ARC/poly_ker_qq.png}
\end{figure}

The polynomial kernel for degrees greater than 1 yields null
distributions with fatter tails than the standard Normal.
\clearpage

Here we look at the effects of both dimension of the underlying data
and degree of the inhomogeneous kernel:
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/ARC/multivar_poly_ker_qq.png}
\end{figure}
\clearpage

\section{Generalizations (Approximate Regression Condition)}
Here we plot $T'$ on $T$, where $T'$ results from swapping labels and
refitting the SVM.  Note that here we don't assign the statistics
random signs because there is no clear way to maintain the coupling
between $T$ and $T'$.

The dimensions (rows) are 1, 10, and 100, and the sample sizes
(columns) are 10, 100, and 1000.
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/ARC/multivar_ARC.png}
\end{figure}

Across a row, it is clear that a larger sample size decreases the
variability of $T'$ about $T$.  And down a column, it is clear that
increasing dimensionality results in a greater departure from (folded)
Normality.  It is not clear whether the reduction in variability is of
order $1/N$.
\clearpage

Here we look at sample sizes (columns) of 10, 100, and 200 and
inhomogeneous polynomial kernel degrees (rows) of 1, 2, and 5.
\begin{figure}[!ht]
  \centering
  \includegraphics{./simulations/ARC/poly_ker_ARC.png}
\end{figure}

We again observe an approximately linear plus noise relationship
between $T'$ and $T$ with the noise decreasing in sample size.

\section{Better Rate}
We observe two samples with equal sample size: $S_1 = \{u_i\}_{i=1}^N$ and $S_2 =
\{u_i\}_{i=N+1}^{2N}$.
Student's two-sample $t$-statistic is given by
\begin{align*}
T_{\Pi}(\{u_{\Pi(i)}\}_{i=1}^N, \{u_{\Pi(i)}\}_{i=N+1}^{2N})
&= \frac{\bar{u}_{1,\Pi} - \bar{u}_{2,\Pi}}{\sqrt{\frac{\frac{1}{N-1}
      \sum_{i=1}^N(u_{\Pi(i)} - \bar{u}_{1,\Pi})^2}{N} + \frac{\frac{1}{N-1}
      \sum_{i=N+1}^{2N}(u_{\Pi(i)} - \bar{u}_{2,\Pi})^2}{N}}} \\
\end{align*}

We need to set $\delta = \max_{\pi, i, j} |T_{\pi} - T_{\pi \circ (i, j)}|$
so that the bound is tight.  This appears to be a daunting
optimization problem.  There are $(2N)!$ permutations and $N^2$
possible transpositions $(i, j)$ for each permutation.  Well, because the
$t$-statistic is invariant to permutations within groups, there are
$\binom{2N}{N}$ (really, $\binom{2N}{N} / 2$ because of symmetry)
permutations to consider.  And there are probably some tricks we can
apply to reduce the $N^2$.  But this still doesn't seem to be very
tractable.

\section{$T$ and $T'$}
Let's first plot all possible values of $T$, and the corresponding
value of $T'$ that maximizes $|T-T'|$.  Here, we make $N$ draws of
sample 1 from $\mathcal{N}(\mu, 1)$ and $N$ draws of sample 2 from
$\mathcal{N}(0, 1)$, varying $N \in \{5, 6, 7\}$ and $\mu \in \{1, 2,
5\}$, coloring the $(T, T')$ pair that maximizes $|T-T'|$.  There is
some symmetry (the pair shows up 4 times) because of swapping $T$ with
$-T$ (2 swaps) and $T'$ with $T$ (2 swaps).

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.12]{./simulations/better_bound_condition/t_tprime_plot.png}
\end{figure}

Unfortunately, we can't make $N$ much bigger than 7 using the current
technique.
\clearpage

\section{Shortcut}
It always seems to be the case that, say, the minimum (equivalently,
the maximum due to symmetry) value of $T_{\pi}$ maximizes $|T - T'|$.
Knowing the permutation $\pi$ that maximizes $|T - T'|$, we can try to
figure out the corresponding transposition $(i, j)$.

It seems reasonable that sorting the data into its order statistics
$\{u_{(i)}\}_{i=1}^{2N}$ will minimize $T$.  That is, let the $N$
smallest be in the first group and the next $N$ in the second.

Another thing that seems reasonable to find the $(i, j)$ that maximizes
$|T - T'|$ is to swap the ``most different'' sample of the first group
with that of the second group: $u_{(1)}$ with $u_{(2N)}$.

I tried this shortcut (it only really involves sorting the data) and
compared it with the exact methodology of the above section and found
agreement in all the tested settings.  This lets us really ramp up $N$
in our simulations.

I haven't really tried to prove it yet: it looks challenging.

Consider drawing sample 1 from $\mathcal{N}(2, \sigma^2)$ and sample 2
from $\mathcal{N}(0, \sigma^2)$ where $\sigma = 1$:
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.06]{./simulations/better_bound_condition/rate_plot_1.png}
\end{figure}

Now consider $\sigma = \sqrt{N}$ (so the power is constant in the
sample size):
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.06]{./simulations/better_bound_condition/rate_plot_2.png}
\end{figure}

Finally, $\sigma = \frac{1}{\sqrt{N}}$:
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.06]{./simulations/better_bound_condition/rate_plot_3.png}
\end{figure}

The last situation is one of those pathological cases that we were
trying to avoid, where the within-group variance vanishes so the
distributions tend toward point masses.  But the other reasonable
cases look good.  That is, if the shortcut works, it appears that
$\delta = \max_{\pi, i, j} |T_{\pi} - T_{\pi \circ (i, j)}|$ is
$O(N^{-1/2})$ for these reasonable cases.
