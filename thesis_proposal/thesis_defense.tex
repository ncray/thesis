\documentclass{beamer}
\mode<presentation> { \setbeamercovered{transparent} }
\setbeamertemplate{navigation symbols}{}
%\setbeamerfont{frametitle}{size=\tiny}
\usetheme{CambridgeUS}
\DeclareGraphicsExtensions{.pdf, .jpg, .gif, .bmp}
\usepackage{amsmath,amsthm}
%\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{appendixnumberbeamer}
%\def\newblock{} %this is needed for natbib
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\law}{\overset{D}{\rightarrow}}
\newcommand{\prob}{\overset{P}{\rightarrow}}
\providecommand{\norm}[1]{\lVert#1\rVert}
\makeatletter
\def\beamerorig@set@color{%
  \pdfliteral{\current@color}%
  \aftergroup\reset@color
}
\def\beamerorig@reset@color{\pdfliteral{\current@color}}
%\newtheorem{proposition}{Proposition}
%\newtheorem{theorem}{Theorem}

\begin{document}
\title[Topics in Two-Sample Testing]{Topics in Two-Sample Testing}
\author[N. Ray]{Nelson Ray \\
  (joint work with Susan Holmes)}
\institute[Stanford]{Stanford University}
\date{April 1, 2013}

\begin{frame}
  \titlepage
\end{frame}

\section{Introduction}
\begin{frame}{Motivation}
  American Gut Study: \url{www.indiegogo.com/americangut}
  \begin{figure}
    \centering
    \includegraphics[scale=.5]{american_gut.png}
  \end{figure}
\end{frame}

\begin{frame}{Breast Cancer Data: Spatial}
  \begin{figure}
    \centering
    \includegraphics[scale=.5]{Fig2healthyDC.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Breast Cancer Data: Survival}
  \begin{figure}
    \centering
    \includegraphics[scale=.5]{survival.png}
  \end{figure}
\end{frame}

\begin{frame}{Breast Cancer Data: Medical}
  \begin{figure}
    \centering
    \includegraphics[scale=.5]{medical.png}
  \end{figure}
\end{frame}

\begin{frame}{Outline}
  \begin{itemize}
  \item Motivation: heterogeneous data are ubiquitous. \pause
  \item Friedman's two-sample test: leverage regression and classification techniques. \pause
  \item Kernel methods for non-vectorial and heterogeneous data. \pause
  \item Generalizes permutation $t$-test! \pause
  \item Stein's method of exchangeable pairs for Berry--Esseen-type bound.
  \end{itemize}
\end{frame}

\begin{frame}{Friedman's Two-Sample Test}
  Friedman (2003) \\
  $\{\mathbf{x}_i\}_{i=1}^n$ from $p(\mathbf{x})$ and
  $\{\mathbf{x}_i\}_{i=n+1}^{n+m}$ from $q(\mathbf{x})$ testing \\
  $\mathcal{H}_A$: $p \neq q$ against $\mathcal{H}_0$: $p = q$ \pause
  \begin{enumerate}
  \item Label the first group $y_i = 1$ and the second group $y_i = -1$ . \pause
  \item Score the observations $\{s_i := f(\mathbf{x}_i)\}_1^{n+m}$ with a learning machine $f$. \pause
  \item Calculate a univariate two-sample test statistic
    $T = T(\{s_i\}_1^n,\{s_i\}_{n+1}^{n+m})$. \pause
  \item Conduct statistical inference based on the permutation null distribution of the above
    statistic.
  \end{enumerate}
\end{frame}

\begin{frame}{Twitter Example}
  \begin{figure}[!ht]
   \centering
   \includegraphics[scale=.3]{pres3.png}
   \includegraphics[scale=.3]{pres4.png}
 \end{figure}
\end{frame}

\begin{frame}[fragile]{Non-vectorial Data}
\begin{verbatim}
"BarackObama: We need to reward education reforms that are
driven not by Washington, but by principals and teachers and
parents. http://OFA.BO/6p2EMy"
\end{verbatim}
\pause

$\bar{x} = $ ? \\ \pause
$\hat{\sigma}_x = $ ? \\ \pause
Kernel methods allow us to lift ourselves up into an inner product space, where we can perform geometric calculations.
\end{frame}

\section{Kernel-based Tests}
\begin{frame}{Kernel Methods}
  The Kernel Trick (Aizerman et al.\ 1964) \pause
  \begin{itemize}
  \item Data $x_i$ in a general set $\mathcal{X}$. \pause
  \item Define a feature map $\phi : \mathcal{X} \to \mathcal{H}$, where $\mathcal{H}$ is a Hilbert space. \pause
  \item $K({\bf x_i}, {\bf x_j}) = \langle \phi({\bf x_i}), \phi({\bf x_j}) \rangle_{\mathcal{H}}$ \pause
  \item Use learning algorithms that only require inner products between vectors in $\mathcal{X}$. \pause
  \item The inner products can be done implicitly, by a kernel function $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Twitter Data}
  Raw:
\begin{verbatim}
"BarackObama: We need to reward education reforms that are
driven not by Washington, but by principals and teachers and
parents. http://OFA.BO/6p2EMy"
"SarahPalinUSA: You betcha!! MT \"@AlaskaAces: Alaska Aces
are 2011 Kelly Cup Champs w/ 5-3 win over Kalamazoo Wings!
Aces win  ECHL Championship series 4-1\""
\end{verbatim}
  After pre-processing:
\begin{verbatim}
"we need to reward education reforms that are driven not by
washington but by principals and teachers and parents "
"you betcha mt alaskaaces alaska aces are  kelly cup champs
w  win over kalamazoo wings aces win  echl championship
series "
\end{verbatim}
\end{frame}

\begin{frame}{The Spectrum Kernel}
  The Spectrum Kernel (Leslie 2002) \\ \pause
  Compares two strings based on the their length $k$ contiguous
  subsequences ($k$-mers). \pause
  \begin{itemize}
    \item $\mathcal{X} = $ set of all finite-length sequences from an alphabet $\mathcal{A}$. \pause
    \item $\phi_2({\bf x}) = [\#_{\text{aa}}({\bf x}), \#_{\text{ab}}({\bf x}), \#_{\text{ac}}({\bf x}), \ldots ]$ \pause
    \item $\mathcal{H} = \mathbb{R}^{|\mathcal{A}|^k}$ \pause
    \item $K_k({\bf x_i}, {\bf x_j}) = \langle \phi_k({\bf x_i}), \phi_k({\bf x_j}) \rangle$
  \end{itemize}
\end{frame}

\begin{frame}{Support Vector Machines}
$\ell_1$-regularized (soft-margin) support vector classification problem
(Vapnik and Cortes, 1995): \pause
\begin{equation*}
\begin{aligned}
& \underset{\mathbf{w} \in \mathcal{H}, b \in \mathbb{R}}{\text{minimize}}
& &\frac{1}{2}\norm{\mathbf{w}}^2+C\sum_{i=1}^{n+m} \xi_i \\
& \text{subject to}
& & y_i(\mathbf{w}^t \mathbf{x}_i + b) \geq 1 - \xi_i \\
&&& \xi_i \geq 0 \qquad \qquad \quad \text{for all } i=1,\ldots,n+m. \pause
\end{aligned}
\end{equation*}
For the Friedman Test, our scoring function is the margin
$f(\mathbf{x}) = \sum_{i=1}^{n+m} y_i \alpha_i K(\mathbf{x}, \mathbf{x}_i) + b$.
\end{frame}

\begin{frame}{KMMD}
  Kernel Maximum Mean Discrepancy Test: (Gretton et al. 2006) \\ \pause
  $\mathfrak{F}$ a class of functions (unit ball in RKHS), $f:\mathcal{X} \to \mathbb{R}$,
  $p$ and $q$ probability distributions, and $X \sim p$ and $Z \sim q$ random variables \\ \pause

  MMD statistic:
  \begin{equation*}
    \text{MMD}[\mathfrak{F},p,q] := \sup_{f\in
      \mathfrak{F}}(\mathbb{E}_{{\bf x} \sim p}[f({\bf x})] - \mathbb{E}_{{\bf z} \sim q}[f({\bf z})]) \pause
  \end{equation*}

  Empirical Estimate:
  \begin{equation*}
    \text{MMD}[\mathfrak{F},X,Z] := \sup_{f\in
      \mathfrak{F}}\left (\frac{1}{N}\sum_{i=1}^Nf({\bf x}_i) -
    \frac{1}{M}\sum_{i=1}^M f({\bf z}_i) \right )
  \end{equation*}
\end{frame}

\begin{frame}{Twitter Example}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../friedman-test/img/power_string.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Image Data (Roosters)}
  Caltech 101 Object Categories (Li et al. 2007) ($297 \times
  300$ grayscale)
  \begin{figure}
    \centering
    \includegraphics[scale=.35]{roosterrs-image_0001.jpg}
    \includegraphics[scale=.35]{roosterrs-image_0002.jpg} \\
    \includegraphics[scale=.35]{roosterrs-image_0003.jpg}
    \includegraphics[scale=.35]{roosterrs-image_0004.jpg}
  \end{figure}
\end{frame}

\begin{frame}{Image Data (Pigeons)}
  \begin{figure}
    \centering
    \includegraphics[scale=.35]{pigeonrs-image_0001.jpg}
    \includegraphics[scale=.35]{pigeonrs-image_0002.jpg} \\
    \includegraphics[scale=.35]{pigeonrs-image_0003.jpg}
    \includegraphics[scale=.35]{pigeonrs-image_0004.jpg}
  \end{figure}
\end{frame}

\begin{frame}{Polynomial Kernel}
  Compares 2 vectors (images) on products of elements (pixel intensities)
  up to a certain order.
  \begin{itemize}
    \item $\mathcal{X} = \mathbb{R}^{p}$ \pause
    \item $\phi_2 ([x_1, x_2]) = [x_1^2,  2x_1x_2,  x_2^2, \sqrt{2c}x_1, \sqrt{2c}x_2, c]$ \pause
    \item $\langle \phi_2({\bf x}), \phi_2({\bf y}) \rangle$ is $\mathcal{O}(n^2)$ \pause
    \item $\mathcal{H} = \mathbb{R}^{p'}$, where $p' = \binom{n+d}{d}$ \pause
    \item $K_d({\bf x},{\bf y}) = ({\bf x}^T {\bf y}  + c)^d$ is $\mathcal{O}(n)$
  \end{itemize}
\end{frame}

\begin{frame}{Rooster/Pigeon Example}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../friedman-test/img/power_birds.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Regression and MKL}
  Regularized regression \pause
  \begin{itemize}
    \item Feature engineering/extraction: ${\bf x}_i$ \pause
    \item Feature normalization: ${\bf x}_i \leftarrow \frac{{\bf x}_i - \hat{\mu}_i}{\hat{\sigma}_i}$ \pause
    \item Regularization/feature selection: $\inf_{{\boldsymbol \beta}} \sum_{i=1}^{m+n} L(\beta_0 + \tilde{{\bf x}}_i^T {\boldsymbol \beta}, y_i) \text{ s.t. } || {\boldsymbol \beta} ||_p \leq t$ \pause
  \end{itemize}
  MKL \pause
  \begin{itemize}
    \item Feature engineering/extraction: $K_i$ \pause
    \item Feature normalization: $K_i({\bf x}, {\bf x}') \leftarrow \frac{K_i({\bf x}, {\bf x}')}{\sqrt{K_i({\bf x}, {\bf x})}\sqrt{K_i({\bf x}', {\bf x}')}}$ \pause
    \item Regularization/feature selection (Kloft et al.\ 2011): $\inf_{{\bf w}, b, {\boldsymbol \theta} : {\boldsymbol \theta} \succeq 0}
      C \sum_{i=1}^{m+n} L( \sum_{j=1}^M \sqrt{\theta_j} \langle {\bf w}_j, \phi_j({\bf x_i}) \rangle_{\mathcal{H}_j} + b, y_i) $ \\
      $\qquad \qquad \; \; \; + \frac{1}{2} \sum_{j=1}^M || {\bf w}_j ||^2_{\mathcal{H}_j}
      \text{ s.t. } || {\boldsymbol \theta} ||_p \leq 1$
  \end{itemize}
\end{frame}

\begin{frame}{Simulated Data (DNA)}
  Generate independent DNA sequences of length $N \sim \text{Pois}(100)$ according to the transition matrix
  \begin{equation*}
    M(p^{\star}) = \bordermatrix{
      ~ & A & C & T & G \cr
      A & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
      C & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} \cr
      T & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} \cr
      G & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
    }
  \end{equation*}
  with stationary distribution $[.25, .25, .25, .25]$. \\ \pause
  $p$ takes $p^{\star} = .25$, and $q$ takes $p^{\star} > .25$. \\ \pause
  $p$ and $q$ generate similar numbers of $1$-mers, but $q$ can generate more AC, CT, TG, GA $2$-mers.
\end{frame}

\begin{frame}{Simulated Data (Star)}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/star.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Two-Sample Tests}
  Two-sample tests typically provide 1 bit of information: accept or reject. \\ \pause

  The MKL-based two-sample test generates the observed kernel weight vector ${\boldsymbol \theta}$ and
  its permuted values ${\boldsymbol \theta}^{(i)}$.
\end{frame}

\begin{frame}{MKL Weights}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/mkl_weights_star_dna.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{MKL Weights}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/mkl_weights_star_dna2.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{MKL Power}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/dna_star_power.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{MKL Null Distribution}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/mkl_null_dist.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{MKL Null Distribution}
  \begin{center}
    \resizebox{10.0cm}{!}{
      \input{../MKL/img/mkl_null_dist_qq.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Permutation $t$-test Connection}
  Want to understand theoretically why the randomization null is normal-like. \pause

  And we wish to derive bounds on
  \begin{equation*}
    \sup_{t \in \mathbb{R}} |P(T_{\Pi} \leq t) - \Phi(t)|,
  \end{equation*}
  where
  \begin{equation*}
    T_{\Pi} = T_{\Pi} \left (\{f({\bf x}_{\Pi(i)})\}_{i=1}^{n}, \{f({\bf x}_{\Pi(i)})\}_{i=n+1}^{2n} \right). \pause
  \end{equation*}

  If $f(x)$ is affine, we recover the permutation $t$-test. \pause

  SVM with linear kernel: $f(x) = \sum_{i=1}^{2n} y_i \alpha_i xx_i + b$.
\end{frame}

\section{Rate of Convergence Bounds}
\begin{frame}{Other Work}
  \begin{itemize}
  \item Fisher (1935) proposed distribution-free
    randomization test.  \pause
  \item Lehmann proved a normal convergence
    result for the randomization distribution. \pause
  \item Bentkus et al. (1996), Shao (2005) proved Berry--Esseen bounds for
    Student's $t$-statistic in independent case.
  \end{itemize}
\end{frame}

\begin{frame}{Other Results}
  \begin{theorem}[Berry--Esseen 1941, 1942]
  Suppose $X_1, \ldots, X_n$ are i.i.d. random variables with
  $\E X_i = 0$, $\E X_i^2 = \sigma^2 > 0$, and $\E |X_i|^3 = \rho
  < \infty$.  Let $F_n(x)$ denote the CDF of standardized sample mean
  of the $X_i$.  Then
  \begin{align*}
    \sup_x |F_n(x) - \Phi(x)| &\leq \frac{0.33477(\rho + 0.429\sigma^3)}{\sigma^3 \sqrt{n}} \\ \pause
    &= \frac{C}{\sqrt{n}} f(\rho, \sigma). \pause
  \end{align*}
  \end{theorem}
  Note that $\rho$ and $\sigma$ are fixed as $n \to \infty$.
\end{frame}

\begin{frame}{Other Results}
  \begin{theorem}[Hoeffding 1951, Stein 1986]
    Let $A = \{a_{ij}\}_{i, j \in \{1, \ldots, n\}}$ be a square array of
      numbers such that $\sum_j a_{ij} = 0$ for all $i$, $\sum_i
      a_{ij} = 0$ for all $j$, and $\sum_i \sum_j a_{ij}^2 = n - 1$.
      Then with $F_n(x) = P(\sum_i a_{i\Pi(i)} \leq x)$,
  \begin{align*}
    \sup_x |F_n(x) - \Phi(x)| &\leq \frac{C}{\sqrt{n}}
    \left (
      \sqrt{\sum_{i,j}a_{ij}^4} + \sqrt{\sum_{i,j}|a_{ij}|^3}
    \right ) \\ \pause
    &= \frac{C}{\sqrt{n}} f(A). \pause
  \end{align*}
  \end{theorem}
  Given a sampling scheme for $A$, $f(A)$ must be $\mathcal{O}(1)$ to have rate $\mathcal{O}(n^{-1/2})$.
\end{frame}

\begin{frame}{Exchangeable Pair}
  Assume $m = n$.  Fix data $\{u_1, \ldots, u_n, u_{n+1}, \ldots, u_{2n}\}$.  $\Pi$ is a uniformly random permutation, and let
  \begin{equation*}
    T_{\Pi} = T_{\Pi} \left (\{u_{\Pi(i)}\}_{i=1}^{n},
      \{u_{\Pi(i)}\}_{i=n+1}^{2n} \right).
  \end{equation*}
  \pause

  Let $(I, J) = (i, j)$ w.p. $\frac{1}{n^2}$ for $1 \leq i \leq n$ and
  $n + 1 \leq j \leq 2n$.  Then
  \begin{equation*}
    T' = T \left (\{u_{\Pi \circ (I, J) (i)}\}_{i=1}^{n},
      \{u_{\Pi \circ (I, J) (i)}\}_{i=n+1}^{2n} \right).
  \end{equation*}
  \pause

  $T$ and $T'$ form an exchangeable pair.
\end{frame}

\begin{frame}{Main Theorem}
\begin{theorem}
  If $T$, $T'$ are mean 0, exchangeable random variables with variance
  $\E[T^2]$ satisfying
  \begin{equation*}
    \E[T'-T|T] = -\lambda(T-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$, then
  $\sup_{t \in \mathbb{R}} |P(T \leq t) - \Phi(t)|$ is bounded by
  \begin{align*}
    \onslide<2->{&\color{red}{\underbrace{(2\pi)^{-1/4} \sqrt{\frac{\E |T'-T|^3}{\lambda}}}_{\leq n^{-1/4} f_1({\bf x})}}}
    \onslide<3->{
    \color{black}{+ \underbrace{\frac{1}{2\lambda} \sqrt{\var (\E [(T'-T)^2|T])}}_{\leq n^{-1} f_2({\bf x})}} \\
      &\underbrace{|\E T^2 - 1|}_{\leq n^{-1} f_3({\bf x})} + \underbrace{\E |TR|}_{\leq n^{-1/2} f_4({\bf x})} +
      \underbrace{\E |R|}_{\leq n^{-1/2} f_5({\bf x})}
    }
    \onslide<3->{\leq n^{-1/4} f_6({\bf x})}
  \end{align*}
\end{theorem}
\end{frame}

\begin{frame}{Main Theorem (Improved Rate)}
\begin{theorem}
  If in addition $|T'-T| \leq \delta$,
  $\sup_{t \in \mathbb{R}} |P(T \leq t) - \Phi(t)|$ is bounded by
  \begin{align*}
    \onslide<2->{&\color{red}{\underbrace{\frac{.41 \delta^3}{\lambda}}_{\leq n^{-1/2} c''^\ast_1}} \color{black}{+} \,
    \color{red}{\underbrace{3 \delta (\sqrt{\E T^2} + \E |R|)}_{\leq n^{-1} f'_1({\bf x})^\ast}} \,}
    \onslide<3->{
      \color{black}{+ \underbrace{\frac{1}{2\lambda} \sqrt{\var (\E [(T'-T)^2|T])}}_{\leq n^{-1} f_2({\bf x})}} \\
      &\underbrace{|\E T^2 - 1|}_{\leq n^{-1} f_3({\bf x})} + \underbrace{\E |TR|}_{\leq n^{-1/2} f_4({\bf x})} +
      \underbrace{\E |R|}_{\leq n^{-1/2} f_5({\bf x})}
    }
    \onslide<3->{\leq n^{-1/2} f'_6({\bf x})^\ast}
  \end{align*}
\end{theorem}
${}^\ast$ if $\delta < c'_1 n^{-1/2}$
\end{frame}

\begin{frame}{Simulated Bounds}
  \begin{center}
    \resizebox{9.0cm}{!}{
      \input{../simulations/img/orig_rate.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Simulated Bounds (Improved Rate)}
  \begin{center}
    \resizebox{9.0cm}{!}{
      \input{../simulations/img/better_rate.tex}
    }
  \end{center}
  \pause
  When ${\bf u} = \{i\}_{i=1}^{i=2n}, \frac{.41 \delta^3}{\lambda}n^{1/2} \to .205(16\sqrt{6})^3$
\end{frame}

\begin{frame}{Behavior of $\delta$}
  \begin{center}
    \resizebox{9.0cm}{!}{
      \input{../simulations/img/delta_plot.tex}
    }
  \end{center}
\end{frame}

\begin{frame}{Conclusion}
  \begin{itemize}
  \item Friedman's test for non-vectorial and heterogeneous data. \pause
  \item MKL can learn structure of data. \pause
  \item MKL power competitive with best-performing kernel and obviates multiple testing considerations. \pause
  \item Normal-like null distributions. \pause
  \item Berry--Esseen-type convergence result via Stein's method of exchangeable pairs.
  \end{itemize}
\end{frame}

\appendix

\section{Acknowledgements}
\begin{frame}{Acknowledgements}
  \begin{figure}
    \centering
    \includegraphics[scale=.39]{susan_blackboard.png}
  \end{figure}
\end{frame}

\begin{frame}{Acknowledgements}
  \begin{figure}
    \centering
    \visible<1->{\includegraphics[scale=.5]{carlsson_blackboard.png}}
    \visible<2->{\includegraphics[scale=.5]{diaconis_blackboard.png}} \\
    \visible<3->{\includegraphics[scale=.5]{efron_blackboard.png}}
    \visible<4->{\includegraphics[scale=.5]{friedman_blackboard.png}}
  \end{figure}
\end{frame}

\section{Supplementary Slides}
\begin{frame}{KMMD Function Example}
  \begin{figure}
    \centering
    \includegraphics[scale=.45]{kmmd.png}
  \end{figure}
\end{frame}

\begin{frame}{Approximate Regression Condition}
  \begin{figure}
    \centering
    \resizebox{9.0cm}{!}{
      \input{../simulations/img/ARC.tex}
    }
  \end{figure}
\end{frame}

\begin{frame}{ARC (MKL)}
  \begin{figure}
    \centering
    \resizebox{9.0cm}{!}{
      \input{../MKL/img/mkl_arc.tex}
    }
  \end{figure}
\end{frame}

\begin{frame}{Overfitting on Kernels}
  \begin{figure}
    \centering
    \resizebox{9.0cm}{!}{
      \input{../MKL/img/overfit_mkl.tex}
    }
  \end{figure}
\end{frame}

\begin{frame}{ARC (MKL, Overfit)}
  \begin{figure}
    \centering
    \resizebox{9.0cm}{!}{
      \input{../MKL/img/mkl_arc_overfit.tex}
    }
  \end{figure}
\end{frame}

 % \begin{frame}[allowframebreaks]{References}
 %   \bibliographystyle{ieeetr}
 %   \bibliography{ncray}
 % \end{frame}

\end{document}
