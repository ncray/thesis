\graphicspath{{./friedman-test/img/}}
\chapter{Friedman's Test}
\label{C:friedman-test}
In this chapter we describe Friedman's approach to the two-sample
problem, provide examples using a kernel support vector machine (KSVM), and
explain the connection between the KSVMs and the theory developed in
Chapter~\ref{C:stein-proof}.

\section{Motivation}
The two-sample problem addresses the issue of comparing samples from
two possibly different probability distributions.  They range from
simple parametric, location alternative tests on univariate data such as the
$t$-test to more general non-parametric, asymptotically consistent tests, which
have power against all alternatives.  Many options exist for vectorial
data, and kernels provide an enticing avenue for extensions to more
general data types.

The two-sample problem is also widely prevalent: ensuring
cross-platform compability of microarray data allows for the merging
samples to achieve larger sample sizes.  Biologists would like to know
whether gene expression levels on a set of genes differ between cancer
and control groups.  Further uses for two-sample testing include
authorship validation: Given two sets of documents, is the hypothesis
of a single author consistent with the data?

\section{Two-Sample Tests}
The two-sample problem is generally posed in the following fashion: $\{
\mathbf{x}_i\}_1^n$ are drawn from $p(\mathbf{x})$ and
$\{\mathbf{y}_i\}_1^m$ are drawn from $q(\mathbf{y})$, where $\mathbf{x}_i,
\mathbf{y}_i \in \mathbb{R}^p$. The goal is to test $H_0:
p(\mathbf{x}) = q(\mathbf{y})$ against $H_A: p(\mathbf{x}) \neq
q(\mathbf{y})$. An ideal test should have power against all
alternatives. That is, as $n,m \to \infty$, the test
will always reject when $p \neq q$ for any non-zero significance level
$\alpha$.

The $t$-test possesses a storied history, with its genesis at Guinness
Brewery in Dublin \cite{box1987guinness}.  At the time, Guinness made
a habit of employing the finest chemistry graduates of Oxford and
Cambridge as brewers and managers, among whom was W.S. Gosset, better
known as Student.  Gosset, appointed brewer in 1899, became
Brewer-in-Charge of the Experimental Brewery and was tasked with
analyzing the results of barley-breeding experiments.  After
consulting with Karl Pearson, Gosset worked on determining the
probable error of the mean and tabulated these results for various
sample sizes.

Gosset successfully analyzed the factors that influenced barley yield
and quality.  As a result, Guinness purchased as much Danish Archer
barley seed as possible and allowed Gosset to publish his methodology
under the pseudonym Student in \cite{student1908probable} and
\cite{student1908probable2}.

Fisher later developed a non-parametric, permutation-based two-sample
test \cite{fisher1935design}.  Hotelling
\cite{hotelling1931generalization} devised a parametric, multivariate
generalization of the $t$-test.  Friedman and Rafsky
\cite{friedman1979multivariate} used minimum spanning trees to extend
the non-parametric Kolmogorov-Smirnov test \cite{smirnoff1939estimation}
to a multivariate setting.  In Section~\ref{S:MMD}, we describe the
Maximum Mean Discrepancy test of Gretton et al.\ \cite{gretton19m},
the first kernel-based two-sample test.

% There are many two-sample tests in the literature, as
% Table~\ref{tab:twosampletests} illustrates.

% \begin{table}
% \centering
% %\begin{tabular}{r|c|c|}
% \begin{tabular}{r | p{5cm} | p{5cm} |}
% \multicolumn{1}{r}{}
%  & \multicolumn{1}{c}{parametric}
%  & \multicolumn{1}{c}{non-parametric} \\
% \cline{2-3}
% univariate & $t$-test & permutation $t$-test; Kolmogorov-Smirnov
% test; Wilcoxon rank-sum test \\
% \cline{2-3}
% multivariate & Hotelling's $T^2$ test & Friedman-Rafsky test \\
% \cline{2-3}
% non-vectorial & Maximum Mean Discrepancy (asymptotic) & Friedman's test (KSVM);
% MMD (distribution-free) \\
% \cline{2-3}
% heterogeneous & & Friedman's test (MKL) \\
% \cline{2-3}
% \end{tabular}
% \caption{Two-sample tests.}
% \label{tab:twosampletests}
% \end{table}

\section{The Friedman Two-Sample Test}
Friedman proposed the following approach to the two-sample problem
\cite{friedman30908multivariate}:

For $\{\mathbf{x}_i\}_1^n$ drawn from $p(\mathbf{x})$ and
$\{\mathbf{x}_i\}_{n+1}^{n+m}$ drawn from $q(\mathbf{x})$, we would like to
test $\mathcal{H}_A$: $p \neq q$ against $\mathcal{H}_0$: $p = q$.
\begin{enumerate}
\item Assign a response value $l_i = 1$ to the observations from the
  first sample ($1 \leq i \leq n$) and $l_i = -1$ to the observations
  from the second sample ($n + 1 \leq i \leq n+m)$.
\item Apply a binary classification learning machine to the training
  data to produce a scoring function $f(\mathbf{x})$ to score each of
  the observations $\{s_i = f(\mathbf{x}_i)\}_1^{n+m}$. \label{item:friedman-score}
\item Calculate a univariate two-sample test statistic $\hat{t} =
  T(\{s_i\}_1^n,\{s_i\}_{n+1}^{n+m})$.
\item Determine the permutation null distribution of the above
  statistic to yield a p-value.
\item The test rejects $\mathcal{H}_0$ at significance level $\alpha$
  if $p < \alpha$.
\end{enumerate}

Note that in Step~\ref{item:friedman-score}, for a given learning
machine, there can still be some choice in the scoring function
$f(\mathbf{x})$.  In Section~\ref{S:SVM-section}, we shall consider
the merits of different scoring functions $f$ producing
correspondingly different scores $s_i$.  Although Friedman suggested
using the $t$-statistic, other statistics are valid as well such as
the Kolmogorov--Smirnov statistic \cite{KolmogorovAN1933, MR0025109}.
Given that the $t$-statistic has no power to detect differences in
variance, sensitivity to such a change must come from the score
mapping $f$.  For instance, the mapping $f(x) = x^2$ would imbue the
$t$-statistic with the power to detect variance shifts.

The Friedman test (FT) is a simple, elegant idea that leverages the
many advancements made over the past several decades in the fields of
prediction and classification and applies them to the problem of
two-sample testing.  In short, as long as there exists a learning
machine for the problem at hand, the Friedman test provides a recipe
for turning that learning machine into a two-sample test.  This
immediately yields two-sample tests for many kinds of data, including
all types for which kernels have been defined.
But there still remains some choice in the scoring
function $f(\mathbf{x})$.  It must be flexible enough to discriminate
between the potential distributional differences of the problem at
hand.  The operating characteristics of the new two-sample test is
\emph{solely} a function of the paired learning algorithm.  For an
excellent overview of many statistical learning algorithms, see
Hastie et al.\ \cite{hastie2009elements}.

By virtue of its permutation construction, the test has level $\alpha$---the
probability that we reject the null hypothesis given
that the null hypothesis is true, also known as type I error.  Given a
threshold $\alpha$, we wish to minimize the type II error, accepting
the null hypothesis given that the alternative hypothesis is true.
Equivalently, we wish to maximize the power, one minus the type II
error \cite{lehmann2005testing}.  The downside of the permutation
design is, of course, that any computational cost is na\"\i vely
multiplied by the number of permutations.  However, there are many
situations for which the cost is sublinear in the number of
permutations.  For instance, caching the computation of the kernel matrix
yields substantial savings when re-using it for permutation based
inference.  This is especially true when computation of the kernel
matrix is expensive relative to finding the SVM parameters via
quadratic programming.

The exact randomization distribution will be a complicated, discrete distribution
parametrized by the observed data.  If, however, we can approximate this
distribution with a simpler one and derive error bounds on the difference
between the two distributions in some probability metric, then we can
use the target distribution as a basis for inference.  We will gain in
computational efficiency by only having to compute the test statistic once.

\section{Kernel Methods}
There exist many two-sample tests for vectorial data $\mathbf{x}_i \in
\mathbb{R}^p$.  Increasingly, data collected for many applications is
heterogeneous in nature and include non-vectorial components such as
text, audio, or graph structures for which the mathematical and
geometric operations required of many learning algorithms are not
defined.  Kernel methods allow us to identify a mapping of the data
from a general set into a Hilbert space in which we can apply certain
classes of algorithms.  For instance, Haussler
\cite{Haussler1999Convolution} developed ways of constructing kernels
between discrete structures such as strings, trees, and graphs.  There
is much literature on kernel methods, but one particularly
comprehensive treatment is the monograph by Sch{\"o}lkopf and Smola
\cite{scholkopf2002learning}.

Given $n$ observed datapoints in some general set, ${\mathbf x}_1, \ldots,
{\mathbf x}_n \in \mathcal{X}$,
kernelized learning algorithms depend only on the pairwise ``similarities'' between
any two observations by way of the kernel function.  Thus, kernel methods effectively
decouple the algorithm (e.g. a support vector machine) from the representation
of the data (e.g. a particular kernel).
\begin{definition}[Positive Semidefinite Kernel]
\label{D:kernel}
A function $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a positive
semidefinite kernel iff it is symmetric ($K(\mathbf{x}, \mathbf{x}') = K(\mathbf{x}', \mathbf{x})$ for any
$\mathbf{x}, \mathbf{x}' \in \mathcal{X}$) and positive semidefinite:
\begin{equation*}
  \sum_{i=1}^n\sum_{j=1}^nc_ic_jK(\mathbf{x}_i, \mathbf{x}_j) \geq 0
\end{equation*}
for any $n > 0$, any choice of $n$ objects $\mathbf{x}_1, \ldots, \mathbf{x}_n \in \mathcal{X}$,
and any $c_1, \ldots, c_n \in \mathbb{R}$.
\end{definition}

Because inner products are symmetric, positive semidefinite functions, they
satisfy Definition~\ref{D:kernel} and are valid kernels.  When $\mathcal{X} = \mathbb{R}^p$,
the linear kernel is defined as
\begin{equation*}
  K(\mathbf{x}, \mathbf{x'}) = \langle \mathbf{x}, \mathbf{x'} \rangle
\end{equation*}
for $\mathbf{x}, \mathbf{x'} \in \mathcal{X}$.

For objects in a general set $\mathcal{X}$, we can define a mapping $\phi$ into a
Hilbert space $\mathcal{H}$ for which an inner product exists.  As there are
many such mappings, one challenge is to choose one that is maximally
useful in exploiting the structure of the data for the task at hand.  In
Chapter~\ref{C:MKL}, we shall explore a technique to identify the most useful
mapping given some parametrized space of mappings.

In fact, for every kernel $K$ we can identify a feature mapping into a Hilbert
space, where the kernel can be expressed as the inner product of the mapped
features \cite{scholkopf2004kernel}:
\begin{theorem}
  For any kernel $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$, there
  exists a Hilbert space $\mathcal{H}$ and a feature mapping
  $\phi: \mathcal{X} \to \mathcal{H}$ such that
  \begin{equation*}
    K(\mathbf{x}, \mathbf{x}') = \langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle_{\mathcal{H}}
  \end{equation*}
  for any $\mathbf{x}, \mathbf{x}' \in \mathcal{X}$, where $\langle u, v \rangle_{\mathcal{H}}$
  represents the inner product in $\mathcal{H}$.
\end{theorem}

In the coming sections and in Chapter~\ref{C:MKL}, we shall see examples of
nonvectorial spaces $\mathcal{X}$, kernels $K$, feature mappings $\phi$, and
Hilbert spaces $\mathcal{H}$.

\section{Support Vector Machines}
\label{S:SVM-section}
A Support Vector Machine (SVM) \cite{cortes1995support} is a supervised learning
technique that seeks to find a hyperplane that maximizes the margin between
points of different classes.  In the case that there exists no separating
hyperplane, a regularization term can be added that controls the effect of
misclassified points.

Although SVMs find linear decision boundaries,
the algorithm depends only on inner products between its datapoints.  The
``kernel trick'' \cite{aizerman1964theoretical} allows us to replace the
inner products with kernel function evaluations, thus effectively finding
a linear decision boundary in the Hilbert space $\mathcal{H}$ identified by
the feature mapping $\phi(x): \mathcal{X} \to \mathcal{H}$.  Although linear
in the typically-higher-dimensional space $\mathcal{H}$, the decision boundary
can be nonlinear in $\mathcal{X}$.

Since kernel methods divorce the representation of the data with the learning
algorithm, development on both fronts can proceed independently.  For instance,
faster optimization algorithms for solving the general SVM problem can proceed
in parallel with the problem-specific designing of new kernels to more efficiently
or effectively exploit the structure of the data.

Consider the $\ell_1$-regularized (soft margin) support vector classification
problem \cite{scholkopf2002learning} in its primal form:
\begin{equation}
\label{eq:l1-svm-primal}
\begin{aligned}
& \underset{\mathbf{w} \in \mathcal{H}, b \in \mathbb{R}, {\boldsymbol \xi} \in \mathbb{R}^{n+m}}{\text{minimize}}
& &\frac{1}{2}\norm{\mathbf{w}}_2^2+C\sum_{i=1}^{n+m} \xi_i \\
& \text{subject to}
& & y_i(\mathbf{w}^t \mathbf{x}_i + b) \geq 1 - \xi_i \\
&&& \xi_i \geq 0 \qquad \qquad \quad \text{for all } i=1,\ldots,n+m.
\end{aligned}
\end{equation}

There are three obvious possibilities for the Friedman scoring
function $f$:
\begin{enumerate}
\item The predicted class label $f_1(\mathbf{x}_i)
= \text{sign}(\mathbf{w}^t \mathbf{x}_i + b)$.
\item An estimate of the posterior class probability, such as by a
sigmoid (Platt \cite{platt1999probabilistic, lin2007note}) or a
logistic link function (Wahba \cite{wahba1992multivariate,
wahba1999support})
$f_2(\mathbf{x}_i) = \frac{1}{1 + \text{exp}(-(\mathbf{w}^t \mathbf{x}_i + b))}$.
\item The margin $f_3(\mathbf{x}_i) = \mathbf{w}^t \mathbf{x}_i + b$.
\end{enumerate}

Because the predicted class label is simply the sign of the margin, we
lose information about how likely it is for a given observation
to belong to a particular class.  Moreover, given constant within-class
predicted class labels, the sample standard deviation is $0$ and hence
the $t$-statistic is unbounded.

Using, for instance, a logistic link function, $f_2$ has the
interpretability of being a posterior class probability--if one
believes in the probability model--and yields no information loss
since it is simply an invertible function of the margin.  However,
it is typically not a linear function of the margin.

The margin $f_3$ has the advantage of being an affine function of the
data.  In one dimension, $f_3(x_i) = w x_i + b$, and since the
$t$-statistic is invariant (up to sign) to affine transformations of
the data, we can see that using the margin generalizes the permutation
$t$-test in some sense.

In the univariate setting, whether or not the $t$-statistic computed
on the scores $\{f_3(x_i)\}$ agrees with that on the raw data
$\{x_i\}$ depends on $\text{sign}(w)$.  Due to symmetry, in the
permutation null distribution, $w$ is negative with probability $.5$
and positive with probability $.5$.  Thus, the permutation null in
both settings appears to be $t$ and hence asymptotically normal.

\subsection{Kernelized Form}
It is advantageous to treat the dual \cite{boyd2004convex} of
Problem~(\ref{eq:l1-svm-primal}).  Because of strong duality,
the primal and dual solutions are equivalent.
Although both optimization problems
are quadratic programs, there exist fast algorithms such as the
sequential minimal optimization algorithm \cite{platt199912} that
exploit the special structure of the dual problem.

In addition, the dual problem is expressed only in terms of inner
products, $\langle \mathbf{x_i}, \mathbf{x_j} \rangle$.  The kernel trick
\cite{aizerman1964theoretical} amounts to replacing these inner products
with kernel function evaluations, $K(\mathbf{x_i}, \mathbf{x_j})$.  The
dual optimization problem is given by
\begin{equation}
\label{eq:l1-svm-dual}
\begin{aligned}
& \underset{{\boldsymbol \alpha} \in \mathbb{R}^{n+m}}{\text{minimize}}
& & \sum_{i=1}^{n+m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{n+m} \alpha_i \alpha_j y_i y_j K(\mathbf{x_i}, \mathbf{x_j})  \\
& \text{subject to}
& & 0 \leq \alpha_i \leq C \quad \text{for all } i=1,\ldots,n+m \\
& \text{and}
& & \sum_{i=1}^{n+m} \alpha_i y_i = 0.
\end{aligned}
\end{equation}

The Karush--Kuhn--Tucker (KKT) \cite{kuhn1951nonlinear} conditions for optimality imply that
\begin{equation*}
  \mathbf{w} = \sum_{i=1}^{n+m} \alpha_i y_i \mathbf{x}_i.
\end{equation*}
Therefore,
\begin{equation*}
  f(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b = \sum_{i=1}^{n+m} \alpha_i k(\mathbf{x}, \mathbf{x}_i) + b,
\end{equation*}
where $\alpha_i$ are the dual variables.

In fact, there is another view of the SVM problem in the framework of regularized
empirical risk minimization \cite{vapnik1999nature} that will be
useful for Chapter~\ref{C:MKL}.  Define the hinge loss function
\begin{equation*}
  L(y, f(\mathbf{x})) := (1 - yf(\mathbf{x}))_+ := \max(0, 1 - yf(\mathbf{x})).
\end{equation*}
Then the following optimization problem is equivalent to Problem~(\ref{eq:l1-svm-primal}):
\begin{equation}
  \label{eq:hinge-svm}
  \min_{b \in \mathbb{R}, \mathbf{w} \in \mathcal{H}} \sum_{i=1}^{n+m} (1 - y_i(\mathbf{w}^T\mathbf{x}_i + b))_+ +
  \frac{1}{2C} \norm{\mathbf{w}}_2^2.
\end{equation}

It turns out that regularized risk minimization problems admit particularly elegant
solutions, a result owing to Kimeldorf and Wahba \cite{kimeldorf1971some}.  We
present a slightly generalized representer theorem from \cite{scholkopf2002learning}:
\begin{theorem}[Representer Theorem]
  \label{T:representer}
  Let $\Omega:[0, \infty] \to \mathbb{R}$ be a strictly monotonic increasing function,
  $\mathcal{X}$ be a set, and $c: (\mathcal{X} \times \mathbb{R}^2)^{n+m} \to \mathbb{R} \cup \{\infty\}$
  be an arbitrary loss function.  Then each minimizer $f' \in \mathcal{H}$ of the regularized risk
  \begin{equation}
    \label{eq:representer}
    c((\mathbf{x}_1, y_1, f'(\mathbf{x}_1)), \ldots, (\mathbf{x}_{n+m}, y_{n+m}, f'(\mathbf{x}_{n+m}))) + \Omega(\norm{f'}_{\mathcal{H}})
  \end{equation}
  admits a representation of the form
  \begin{equation*}
    f'(\mathbf{x}) = \sum_{i=1}^{n+m} \alpha'_i K(\mathbf{x}_i, \mathbf{x}).
  \end{equation*}
\end{theorem}
Although Problem (\ref{eq:representer}) has a feasible set $\mathcal{H}$
that is possibly infinite dimensional, Theorem~\ref{T:representer} guarantees
that the solution lies in the span of the $n+m$ particular kernels centered
on the observations $\mathbf{x}_i$.

We apply Theorem~\ref{T:representer} to Problem~(\ref{eq:hinge-svm}), noting that it holds
for all fixed $b$, to conclude that
\begin{equation*}
  f'(\mathbf{x}) = \mathbf{w}^T\mathbf{x} = \sum_{i=1}^{n+m} \alpha'_i K(\mathbf{x}_i, \mathbf{x}).
\end{equation*}
Thus, setting $\alpha'_i = y_i \alpha_i$, we again find
\begin{equation*}
  f(\mathbf{x}) = f'(\mathbf{x}) + b = \sum_{i=1}^{n+m} y_i \alpha_i K(\mathbf{x}, \mathbf{x}_i) + b.
\end{equation*}

We list the kernelized representations of possible Friedman scoring functions:
\begin{enumerate}
\item The predicted class label $f_1(\mathbf{x}_i)
= \text{sign}(\sum_{i=1}^{n+m} y_i \alpha_i K(\mathbf{x}, \mathbf{x}_i) + b)$.
\item An estimate of the posterior class probability, such as by a
sigmoid (Platt \cite{platt1999probabilistic, lin2007note}) or a
logistic link function (Wahba \cite{wahba1992multivariate,
wahba1999support})
$f_2(\mathbf{x}_i) = \frac{1}{1 + \text{exp}(-(\sum_{i=1}^{n+m} y_i \alpha_i K(\mathbf{x}, \mathbf{x}_i) + b)}$.
\item The margin $f_3(\mathbf{x}_i) = \sum_{i=1}^{n+m} y_i \alpha_i K(\mathbf{x}, \mathbf{x}_i) + b$.
\end{enumerate}

\subsection{Equivalence to the Permutation $t$-test}
\begin{theorem}
  \label{friedman_equiv}
  The Friedman test paired with support vector regression
  or support vector classification (using the margin as a score)
  with the appropriate kernel generalizes
  the two-sample permutation $t$-test. In particular, the two procedures are
  equivalent with univariate data and a linear kernel.
\end{theorem}

\begin{proof}
  \begin{equation*}
    f(x)=\sum_{i=1}^{n+m} y_i \alpha_iK(x_i,x)+b =
    \left ( \sum_{i=1}^{n+m} y_i \alpha_i x_i \right )x+b = wx+b,
  \end{equation*}
  since we have univariate data and an affine kernel.
  Therefore, the SVM score is simply an affine transformation of the
  data.  Welch's $t$-statistic is given by
  \begin{equation*}
    T(\{x_i\}_1^n,\{x_i\}_{n+1}^{n+m}) = \frac{\bar{x}-\bar{x}'}{\sqrt{\frac{s_X^2}{n}+\frac{s_{X'}^2}{m}}}
  \end{equation*}
  where
  \begin{equation*}
    \bar{x}=\frac{1}{n}\sum_{i=1}^n x_i \text{ and }
    s_X^2= \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2.
  \end{equation*}
  Let $z=f(x)=wx+b$ and note that
  \begin{equation*}
    \bar{z}=\frac{1}{n}\sum_{i=1}^{n} z_i = \frac{w}{n}\sum_{i=1}^n x_i
    + b = w\bar{x}+b
  \end{equation*}
  and
  \begin{equation*}
    s_Z^2= \frac{1}{n-1}\sum_{i=1}^n(z_i-\bar{z})^2= \frac{1}{n-1}\sum_{i=1}^n(wx_i+b-w\bar{x}+b)^2=w^2s_X^2.
  \end{equation*}
  Therefore,
  \begin{equation*}
    T(\{f(x_i)\}_1^n,\{f(x_i)\}_{n+1}^{n+m}) =
    \frac{w\bar{x}+b-w\bar{x}'+b}{|w|\sqrt{\frac{s_{X}^2}{n}+\frac{s_{X'}^2}{m}}}
    = \text{sign}(w) T(\{x_i\}_1^n,\{x_i\}_{n+1}^{n+m}).
  \end{equation*}
  Since we are interested in two-sided testing, we consider
  \begin{equation*}
    |T(\{f(x_i)\}_1^n,\{f(x_i)\}_{n+1}^{n+m})| = |T(\{x_i\}_1^n,\{x_i\}_{n+1}^{n+m})|.
  \end{equation*}
  Thus, the $t$-statistics are identical, and since the permutation
  procedure is the same, the tests are equivalent.

  To see the result for support vector regression,
  recall that support vector regression solves the following problem
  \cite{scholkopf2002learning}:
  \begin{equation*}
    \begin{aligned}
      & \underset{\mathbf{w}\in\mathcal{H},\mathbf{\xi}^{(*)}\in\mathbb{R}^{n+m},b\in\mathbb{R}}{\text{minimize}}
      & &\tau \left ( \mathbf{w},\mathbf{\xi}^{(*)} \right ) =\frac{1}{2}\norm{\mathbf{w}}^2+C\sum_{i=1}^{n+m}(\xi_i+\xi_i^*) \\
      & \text{subject to}
      & & f(\mathbf{x}_i)-y_i \leq \epsilon +\xi_i\\
      &&& y_i-f(\mathbf{x}_i) \leq \epsilon+\xi_i^* \\
      &&& \xi_i,\xi_i^*\geq0 \qquad \qquad \quad \text{for all } i=1,\ldots,n+m.
    \end{aligned}
  \end{equation*}
  with solution is given by
  \begin{equation*}
    f(x)=\sum_{i=1}^{n+m}(\alpha_i^*-\alpha_i)K(x_i,x)+b.
  \end{equation*}
\end{proof}

Using the bounds derived in Chapter~\ref{C:stein-proof}, we can
conduct statistical inference with the normal distribution rather than
trying to compute the randomization distribution.

We have shown that for univariate data, some kernels generalize the
permutation $t$-test.  Is it possible to characterize all such kernels?
For what kernels $K$ do we have
\begin{equation}
\sum_{i=1}^{n+m} y_i \alpha_i K(x, x_i) + b = cx + d?
\end{equation}

A sufficient condition is for $K(x, x_i) = \langle \Phi(x), \Phi(x_i) \rangle =
f(x_i) x$.  The linear kernel satisfies this condition with $f(x_i) =
x_i$.  The RBF kernel $K(x, x_i) = \text{exp}(-\sigma (x-x_i)^2)$ does
not yield an affine function of the data, as
\begin{equation}
\sum_{i=1}^{n+m} y_i \alpha_i \text{exp}(-\sigma (x-x_i)^2) + b
\end{equation}
cannot be written as $cx + d$.

We use Support Vector Machine (SVM) classification
as implemented in the {\bf ksvm} function of the {\bf R}
\cite{cran} package {\bf kernlab} \cite{kernlab}.

The cost parameter $C$ controls the complexity of the prediction
function. It is typically chosen via cross-validation over a grid
of choices.

\section{Maximum Mean Discrepancy}
\label{S:MMD}
Gretton et al.\ \cite{gretton19m, gretton2010fast, gretton2012kernel,
  borgwardt2006integrating} introduced a kernel-based approach for
the two-sample problem based on the Maximum Mean Discrepancy (MMD)
statistic, an integral probability metric.  MMD provides good
performance in practice, strong theoretical guarantees, and is the
first two-sample test for comparing distributions over graphs.

\begin{definition}
  With $\mathfrak{F}$ a class of functions $f:\mathcal{X} \to
  \mathbb{R}$, $p$ and $q$ probability distributions, and $X \sim p$
  and $Z \sim q$ random variables, the maximum mean discrepancy (MMD)
  and an empirical estimate are defined as
  \begin{equation*}
    \text{MMD}[\mathfrak{F},p,q] := \sup_{f\in
      \mathfrak{F}}(\mathbb{E}_{x\sim p}[f(x)] - \mathbb{E}_{z\sim q}[f(z)]),
  \end{equation*}
  \begin{equation*}
    \text{MMD}[\mathfrak{F},X,Z] := \sup_{f\in
      \mathfrak{F}}\left (\frac{1}{n}\sum_{i=1}^nf(x_i) -
    \frac{1}{m}\sum_{i=1}^{m} f(z_i) \right ).
  \end{equation*}
\end{definition}

An unbiased empirical estimate \cite{gretton2012kernel} estimate of
the statistic is given by
\begin{equation*}
  \text{MMD}^2_u[\mathfrak{F},X,Z] :=
  \frac{1}{m(m-1)} \sum_{i=1}^m \sum_{j \neq i}^m K(x_i, x_j) +
  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j \neq i}^n K(z_i, z_j) -
  \frac{2}{mn} \sum_{i=1}^m \sum_{j = 1}^n K(x_i, z_j).
\end{equation*}

The function class $\mathfrak{F}$ is typically taken to be the unit ball in a
reproducing kernel Hilbert space (RKHS), however, well-known metrics
can be obtained over other function classes.  Although Gretton et
al.\ provide several distribution-free tests based on MMD theory, we
instead compare the Friedman test (FT) against the permutation-based
MMD so as to compare statistic with statistic.  In this way, the
theory is dissociated from the comparison.  We feel that this is the
most fair comparison of the two tests because many of the theoretical
results are inexact.  We also do not have large enough sample sizes in
our real datasets to ensure low error in theoretical approximations.
Even if we did, the power for the tests would be very nearly one,
making comparisons on non-simulated data difficult.

The Kernel MMD (KMMD) test seeks ``smooth'' functions that maximize
the difference between the two classes of points, where smoothness
is defined in terms of the Hilbert norm.  This allows for nonlinear
functions $f$ in the feature space, as opposed to the hyperplanes
learned by the SVM algorithm.

\section{Null Distributions}
The null distribution plays a fundamental role in frequentist statistical
inference.  Hotelling's $T^2$-statistic has null distribution that
corresponds to a scaled central $F_{(p, n+m-1-p)}$ distribution, where
$p$ is the dimensionality of the data and $n, m$ are the sample sizes
of the two groups.  As its name suggests, the $T^2$-test is a
generalization of Student's $t$-test, and for $T \sim t(n+m-2)$, we
have that $T^2 \sim F_{(1, n+m-2)}$.  As a consequence of
Theorem~\ref{friedman_equiv}, the Friedman statistic in the univariate
data, linear kernel setting is equal to the $|T|$.  In Figure~\ref{fig:null_dist} we simulate
200 standard multivariate normal draws from each class with dimension
$D \in \{1, 5, 10\}$.  We compare the null distributions of the
$T^2$-statistic, KMMD, and Friedman statistics with a linear kernel
and RBF kernel with width parameter 1.  We draw 5,000 samples from
each permutation null distribution and apply a kernel density smoother
to the results.  It appears that many of the null distributions are
very close to normal (or, $t(398)$, rather).

The Friedman Statistic null distributions appear to be consistent with
a standard normal distribution.  In Figure~\ref{fig:c_param} we
examine the relationship between the Friedman Statistic and the
regularization parameter, $C$.  We have proven that in the univariate
setting with a linear kernel, the statistic is independent of $C$.
Empirically, it appears that for higher dimensional data, $C$ has
a minor effect on the Friedman statistic for the linear kernel.
With an RBF kernel with width 1, it appears that $C$ has a small effect.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{friedman-test/img/c_param.tex}
    }
  \end{center}
\caption{FS.l: FS with a linear kernel; FS.rbf: FS with RBF kernel. We vary
the dimension of the data: 1, 2, 5, 10.}
\label{fig:c_param}
\end{figure}

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{friedman-test/img/null_dist.tex}
    }
  \end{center}
\caption{T2: Hotelling's $T^2$-statistic; sqrtT2: $|T|$;
KMMD.l: kernel MMD with a linear kernel; FS.l: FS with a
linear kernel; KMMD.rbf: kernel MMD with a radial basis function (RBF) kernel;
FS.rbf: FS with RBF kernel}
\label{fig:null_dist}
\end{figure}

The $T^2$ densities correspond to a
parametrized family of $F$-distributions.  It is not surprising that the MMD
linear kernel null distributions shift rightward as a function of
dimension: the higher dimensionality allows the function in the RKHS
to better find discrepancies between the two empirical distributions.
The same rationale holds true for the FS when thinking of separating
hyperplanes.  Interestingly, there are marked differences between the
MMD and FS for the RBF kernel.  Note also that the support of the KMMD
statistic is $\mathbb{R}^+$.

\section{Experiments}
\subsection{Vectorial Data}
We consider $\{x_i\}_{i=1}^{20} \sim \mathrm{MVN}_d(\mathbf{0},
\mathbf{I})$ and $\{y_i\}_{i=1}^{20} \sim
\mathrm{MVN}_d(\Delta \mathbf{1}, \mathbf{I})$ where our
dimensionality $d \in \{1, 5, 10, 20\}$ and mean difference $\Delta \in
\{0, .5, \ldots, 1.5\}$ in Figure~\ref{fig:power_normal}.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{friedman-test/img/power_normal.tex}
    }
  \end{center}
  \caption{FS: Friedman statistic; KMMD: kernel Maximum Mean
    Discrepancy; T2: Hotelling's $T^2$-statistic; Error bars indicate
    95\% bootstrap confidence intervals.}
  \label{fig:power_normal}
\end{figure}

For FS and MMD, we use the the RBF kernel with a width of 1.  The
methods perform similarly with the exception of the kernel methods
using the RBF kernel.  This suggests that either a width of 1 is
ineffective or the RBF kernel is unsuitable for these data.

\subsection{String Data}
\label{twitter_data}
For a string data comparison using data from Twitter, we collected the
latest 1,000 tweets from Barack Obama (@BarackObama) and Sarah Palin
(@SarahPalinUSA) obtained from the {\bf R} package {\bf twitteR}
\cite{twitteR}.  We pre-process and transform the tweets
into an alphabet comprising only lowercase English letters and spaces.
For simplicity, we
choose the $k$-spectrum kernel \cite{leslie2002spectrum} with
$k \in \{1, 2, 3 \}$
as our kernels for both the FT and MMD.

Here is an example of the raw tweets:
\begin{verbatim}
"BarackObama: We need to reward education reforms that are
driven not by Washington, but by principals and teachers and
parents. http://OFA.BO/6p2EMy"
"SarahPalinUSA: You betcha!! MT \"@AlaskaAces: Alaska Aces
are 2011 Kelly Cup Champs w/ 5-3 win over Kalamazoo Wings!
Aces win  ECHL Championship series 4-1\""
\end{verbatim}
And pre-processed tweets:
\begin{verbatim}
"we need to reward education reforms that are driven not by
washington but by principals and teachers and parents "
"you betcha mt alaskaaces alaska aces are  kelly cup champs
w  win over kalamazoo wings aces win  echl championship
series "
\end{verbatim}

For the $k$-spectrum kernel,
\begin{itemize}
\item $\mathcal{X} = $ set of all finite-length sequences from an alphabet $\mathcal{A}$.
\item $\phi({\mathbf x}) = $ the number of length $k$ contiguous
  subsequences ($k$-mers) in ${\mathbf x}$.
\item $\mathcal{H} = \mathbb{N}^{|\mathcal{A}|^k}$.
\item $K_k({\mathbf x}, {\mathbf x}') = \langle \phi_k({\mathbf x}), \phi_k({\mathbf x}') \rangle$.
\end{itemize}

Suffix trees allow for efficient kernel calculations, computing
$K_k({\mathbf x}, {\mathbf x}')$ in
$\mathcal{O}(kn)$ time.

We draw samples of various sizes from
both the Barack Obama tweets and Sarah Palin tweets in order to
empirically determine the power, with results detailed in
Figure~\ref{fig:power_string}.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{friedman-test/img/power_string.tex}
    }
  \end{center}
  \caption{FS: Friedman statistic; KMMD: kernel Maximum Mean
    Discrepancy; Error bars indicate 95\% bootstrap confidence
    intervals.}
  \label{fig:power_string}
\end{figure}

The MMD test outperforms the Friedman test on this task for $k < 3$.  Power
increases as a function of $k$ for both tests, and it is somewhat
surprising to see the strong performance from considering only
frequencies of unigrams.  The KMMD's strong performance is likely
due to the greater flexibility in being able to learn a nonlinear function
in the smaller feature spaces corresponding to $k = 1$ and $2$.  We see
that the advantage largely disappears for $k = 3$.

\subsection{Image Data}
\begin{figure}
  \centering
      \includegraphics[scale=.2]{roosterrs-image_0001.jpg}
      \includegraphics[scale=.2]{roosterrs-image_0002.jpg}
      \includegraphics[scale=.2]{roosterrs-image_0003.jpg}
      \includegraphics[scale=.2]{pigeonrs-image_0001.jpg}
      \includegraphics[scale=.2]{pigeonrs-image_0002.jpg}
      \includegraphics[scale=.2]{pigeonrs-image_0004.jpg}
      \caption{Images of roosters and pigeons for use in
        discrimination test.}
  \label{fig:birds}
\end{figure}

We consider the task of discriminating between images of roosters and
pigeons from the Caltech 101 Object Categories dataset
\cite{fei2007learning}.  Samples of the birds are in Figure~\ref{fig:birds}.
We resize images to a common resolution of $300 \times
297$ and convert to a vector of 8 bit grayscale values.
To correct for global differences in illumination and ensure
that only local patterns would be used for discrimination, we centered and
scaled each vector.  Power comparisons can be seen in Figure~\ref{fig:power_birds}.

For the polynomial kernel,
\begin{itemize}
\item $\mathcal{X} = \mathbb{R}^{n}$.
\item $\phi_2 ([x_1, x_2]) = [x_1^2,  2x_1x_2,  x_2^2, \sqrt{2c}x_1, \sqrt{2c}x_2, c]$.
\item $\langle \phi_2({\mathbf x}), \phi_2({\mathbf x}') \rangle$ is $\mathcal{O}(n^2)$.
\item $\mathcal{H} = \mathbb{R}^{d'}$, where $d' = \binom{n+d}{d}$.
\item $K_d({\mathbf x},{\mathbf x}') = ({\mathbf x}^T {\mathbf x}'  + c)^d$ is $\mathcal{O}(n)$.
\end{itemize}

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{friedman-test/img/power_birds.tex}
    }
  \end{center}
  \caption{KMMD: the kernel Maximum Mean Discrepancy test; FS: the
    Friedman test; Columns indicate the degree of the polynomial
    kernel; Rows indicate the regularization parameter $C$; Error bars indicate
    95\% bootstrap confidence intervals.}
  \label{fig:power_birds}
\end{figure}

Again, MMD performs better.  However, the Friedman test's performance
certainly improves when considering higher degree polynomials.

% \section{Extensions}
% \subsection{Heterogeneous Data}
% This procedure extends naturally to the heterogeneous data setting via
% multiple kernel learning (MKL) \cite{lanckriet2004learning,
%   gonen2011multiple}.  Qiu et al.\ \cite{qiu2005multiple} develop MKL
% for support vector regression.  Given $j$ different data modalities,
% it suffices to match a kernel $K_i$ to each---or perhaps more than one
% kernel for each data source, so as to better target specific features.
% The semidefinite programming approach (SDP) to MKL finds the best
% linear combination $K = \sum_{i=1}^j \mu_i K_i$ for some relevant
% objective function.  For computational reasons, the best non-negative
% linear combination is frequently sought, as this yields a simpler quadratically
% constrained quadratic program (QCQP).

% \subsection{Missing Data}
% If we further consider entire missing modalities (e.g. one sample is
% missing some biometric reading), Poh et al.\ \cite{poh2010addressing}
% develop the \emph{neutral point substitution} technique to allow
% substitution of the missing modality with a new kernel that is
% \emph{unbiased} with regard to the classification at hand.  This
% allows for full use of both modalities that are present for all
% samples as well as those that are present only for a subset of the
% samples and effective utilization of all the data in the training set.
% Panov et al.\ \cite{panov2011modified} modify the NPS method to allow
% for missing modalities in the test set.

% \subsection{Theoretical Guarantees}
% Having proved a bound in the univariate data, linear kernel case by
% constructing an exchangeable pair, Anonymous and Anonymous
% \cite{rayholmes2012} use simulations to suggest
% that the same pair is likely to yield success in more general
% settings: the key \emph{approximate regression condition} holds more
% universally for multivariate data, a non-linear kernel, and a
% combination of the two settings.  Further simulations demonstrate that
% the $\mathcal{O}(N^{-1/4})$ rate of convergence does not appear to be
% tight and a more typical $\mathcal{O}(N^{-1/2})$ is within reach.

% A rate of convergence result with known constant allows
% for a single calculation of the Friedman statistic---rather than the
% $N_{\mathrm{perm}}$ required for randomization-based inference.
% Theoretical inference could be done on the limiting distribution, with
% error characterized by the proven bound.  This large savings in
% computation comes only at the known cost of the limiting distribution
% approximation, which falls rapidly in sample size.

% \section{Discussion}
% We have tested a two-sample testing method of Friedman's
% \cite{friedman30908multivariate} with a particular choice of learning
% algorithm---support vector regression.  This Friedman test can be seen
% as a generalization of the celebrated permutation $t$-test, or
% randomization test.  Without tuning, performance is competitive in
% some settings with the MMD test.  Simulations suggest that more
% powerful tests may be achieved with the added complexity of
% tuning---at some computational cost.  Further work is required to
% determine a good set of heuristic choices for the SVM tuning parameters.

% Modern data sources often consist of different modalities.  Wireless
% sensor networks (including cellular phones) are deployed to collect
% large quantities of \emph{diverse} data.  These networks may be
% heterogeneous, with newer and upgraded hardware logging novel sources
% of data.  Because Friedman's idea leverages \emph{any} learning algorithm, we
% can at present easily incorporate extensions such as both the treatment of
% heterogeneous data \emph{and} an allowance for missing data
% modalities.  Future developments in regression and classification can
% be incorporated to advance the state-of-the-art in two-sample testing.
