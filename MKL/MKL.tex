\graphicspath{{./MKL/img/}}
\chapter{Multiple Kernels}
\label{C:MKL}
In this chapter we introduce a framework for two sample testing based
on heterogeneous data based on multiple kernel learning (MKL).

\section{Introduction}
Given a set of kernels, it is possible to combine them in order to produce new
kernels.  This is a starting point for heterogeneous data analysis: we
can define a kernel $K_i$ for each data domain and develop a kernel $K$ that operates
on the union of the domains.  Rather, we typically shall produce a parametrized
family of kernels $K_{{\boldsymbol \theta}}$ and seek the ``best'' choice of
parameters ${\boldsymbol \theta}$.

For example, the class of kernel functions on $\mathcal{X}$ is closed under pointwise products
(also known as Schur products) of kernels,
\begin{equation*}
  K(x,x') = (K_1K_2)(x,x')=K_1(x,x')K_2(x,x'),
\end{equation*}
tensor products of kernels,
\begin{equation*}
 K(x, x') = (K_1 \otimes  K_2)(x_1,x_2,x_1',x_2')=K_1(x_1,x_1')K_2(x_2,x_2'),
\end{equation*}
and conic combinations of kernels,
\begin{equation*}
  K_{{\boldsymbol \theta}}(x, x') = (\theta_1 K_1 + \ldots + \theta_m K_m)(x,x')=
  \theta_1 K_1(x,x') + \ldots + \theta_m K_m(x,x').
\end{equation*}

An unweighted sum of kernels is equivalent to concatenating the individual
feature spaces.

\section{Multiple Kernel Learning}
In a landmark paper, Lanckriet et al.\ \cite{lanckriet2004learning} showed
that for various SVM objective functions, the problem of finding the optimal
conic combination of kernels could be posed as a convex optimization problem.
Although the initial approach involved solving a computationally expensive
semidefinite program, this sparked a flurry of research on similar convex
approaches to MKL.

Kloft et al.\ \cite{kloft2011lp} conceived a general $\ell_p$-norm approach
to MKL, unifying many special cases and further proposed a highly efficient
algorithm.  This can be seen as generalizing Problem~(\ref{eq:hinge-svm})
of Chapter~\ref{C:friedman-test}.

Let $L: \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ be a loss function,
$\Omega : \mathcal{H} \to \mathbb{R}$ be a regularizer, and $\lambda > 0$
be a tradeoff parameter.

Kloft et al.\ consider linear models of the form
\begin{equation*}
  h_{\tilde{w},b,\theta}(x) = \sum_{i=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m (x) \rangle_{\mathcal{H}_m} + b
  = \langle \tilde{w}, \phi_{\theta}(x) \rangle_{\mathcal{H}} + b,
\end{equation*}
where $\tilde{w} = [\tilde{w}_1^T,\ldots,\tilde{w}_m^T]^T$ and
$\phi_{\theta} = \sqrt{\theta_1} \phi_1 \times \ldots \times \sqrt{\theta_m} \phi_m$.

The regularized risk minimization problem is the following:
\begin{equation}
  \label{eq:mkl-rrm-1}
  \min_{\tilde{w}, b, \theta : \theta \succeq 0} \frac{1}{n} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{\lambda}{2}\sum_{m=1}^M \norm{\tilde{w}_m}_{\mathcal{H}_m}^2 +
  \tilde{\mu} \tilde{\Omega}(\theta),
\end{equation}
for $\tilde{\mu} > 0$.

Problem~(\ref{eq:mkl-rrm-1}) is not convex but can be transformed into a convex
problem via the substitution
\begin{equation*}
  w_m \leftarrow \sqrt{\theta_m} \tilde{w}_m.
\end{equation*}

Decoupling the regularization parameter from the sample size by letting
$\tilde{C} = \frac{1}{n \lambda}$ and $\mu \leftarrow \frac{\tilde{\mu}}{\lambda}$,
and using convex regularizers of the form $\tilde{\Omega}(\theta) = \norm{\theta}_p^2$,
we get
\begin{equation}
  \label{eq:mkl-rrm-2}
  \min_{w, b, \theta : \theta \succeq 0} \tilde{C} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} +
  \mu\norm{\theta}_p^2,
\end{equation}
where $\frac{t}{0} = 0$ if $t=0$ and $\infty$ otherwise.

Kloft et al.\ prove that the Tikhonov-regularized Problem (\ref{eq:mkl-rrm-2})
with two parameters is in fact equivalent to the following Ivanov-regularized
problem with one regularization parameter, $C$:
\begin{equation}
\label{eq:mkl-ivanov}
\begin{aligned}
& \underset{w, b, \theta : \theta \succeq 0}{\text{minimize}}
& & C \sum_{i=1}^n L \left (\sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
\right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} \\
& \text{subject to}
& & \norm{\theta}_p^2 \leq 1.
\end{aligned}
\end{equation}

We use Problem~(\ref{eq:mkl-ivanov}) as implemented in SHOGUN \cite{sonnenburg2010shogun}.

\section{Simulations}
\subsection{Vectorial Data Mixture Distribution}
Let's look at mixtures of MVN.  Let
\begin{align*}
  \Sigma_1 &=
  \begin{bmatrix}
    1^2 & 0   \\
    0   & 1^2 \\
  \end{bmatrix} \\
  \Sigma_2 &=
  \begin{bmatrix}
    10^2 & 0   \\
    0    & 10^2 \\
  \end{bmatrix} \\
  \mu_1(\delta_1) &= [\delta_1, 0]^T \\
  \mu_2(\delta_2) &= [\delta_2, 100]^T. \\
\end{align*}

Let $d_1$ be a mixture distribution of $\mathcal{N}_2([0, 0]^T, \Sigma_1)$ with
probability $p$ and $\mathcal{N}_2([0, 100]^T, \Sigma_2)$ with
probability $1-p$.  Let $d_2$ be a mixture distribution of
$\mathcal{N}_2([1, 0]^T, \Sigma_1)$ with probability $p$ and
$\mathcal{N}_2([10, 100]^T, \Sigma_2)$ with probability $1-p$.  Note
that $\delta_1 = 1$ and $\delta_2 = 10$ were chosen to be one standard
deviation away (on the x-axis, see $(\Sigma_1)_{1,1}$ and
$(\Sigma_2)_{1,1}$).  In all the simulations, we draw $n = 50$ samples
from each mixture distribution, $d_1$ and $d_2$.  We take the mixture
probability $p = .5$ unless otherwise specified.

Here is a plot of the 95\% confidence ellipses of the mixture distributions:
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.3]{vectorial_distributions.png}
\end{figure}

Here we plot the average power and bootstrap 95\% confidence intervals
(100 simulations) for each RBF kernel individually and MKL on all of
them, faceted on the mixture probability.  So for $p = 0$, we have all
the weight on $\mathcal{N}_2(\mu_2, \Sigma_2)$, and for $p = 1$, we
have all the weight on $\mathcal{N}_2(\mu_1, \Sigma_1)$.  Since the
latter is on a smaller scale, we expect the smaller width RBF kernels
to do better.  We take $C = 1$, and the widths to be from the middle
run of the last section:
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.3]{vectorial_power.png}
\end{figure}
The smaller distribution ($p=1$) has higher power for smaller kernels,
but the MKL power is about the same as compared with the $p=0$ case.
Both outperform the mixed setting.

Here are the null distributions of the weights and the observed weights:
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.3]{vectorial_weights.png}
\end{figure}

\subsection{Heterogeneous Data}
I created a test
heterogeneous dataset that has two components to it: DNA string and
univariate.  I created the DNA data via a Markov chain because I
wanted the joint distribution of 2-grams to be different from the
product of two 1-grams.  I wanted this dependence so that I could
later pick out differences between two groups with a 2-spectrum kernel
instead of a 1-spectrum kernel.  For the first group, I randomly
picked a starting string according to the stationary distribution and
then proceeded via the transition probabilities.  For the second
group, I had independent draws from the stationary distribution.

The univariate data is simply $\mathcal{N}(\{-\mu, \mu\}, 1)$, and there are 20
samples in each group.  I fixed the kernel training parameter values
and trained a convex combination of 5 kernels on the data via MKL:
Gaussian RBF kernels with parameter .1, .2, .5, and 1, and a
2-spectrum kernel (later I do want to test that the 2-spectrum is
required in this case over the 1-spectrum because of the Markov chain
construction of the data, but I had to add pre-processors and I'm
still not too familiar with shogun yet).

I looked at the kernel weights (so no Friedman test yet) over $\mu = .5,
.6, \ldots, 2.9, 3$.  The weights on, say, the 2-spectrum kernel aren't
monotonically decreasing because of sampling variance: I only ran each
of these once.  I have attached MKL1.png to show that the signal in
the DNA data outweighs (in the sense of yielding a higher MKL weight)
the signal in the univariate part of the data until $\mu = 1.5$ or so.
At that point, the RBF1 kernel takes over.

The pictures from our meeting weren't that compelling, so I've been
looking for better examples.  I decided a reasonable one was the
Christmas star example on page 1548 of
http://eprints.pascal-network.org/archive/00002269/01/sonnenburg06a.pdf

Imagine an outer star (radius 4, 5, 6, 7, 8) with an inner star
(radius 4) inside.  The different stars correspond to different
labelings in the classification problem.  I looked at 5 kernels
individually (RBF with width .01, .1, 1, 10, 1000) and the MKL
combination of all of them).

Here's a heterogeneous data example.  I added string (DNA) data to the
Christmas star example from last week.  So each point is $(l_i, x_i,
y_i, s_i)$ = (label, Christmas star x-coordinate, Christmas star
y-coordinate, DNA sequence).  I generated the DNA by first picking a
random (Poisson) length, sampling the starting point from the
stationary distribution (all 1/4) of the Markov chain, and then
picking according to the transition matrix $M$, where $M_{i,i} = s$ (for
self transition probability) and $M_{i,j} = \frac{(1 - s)}{3}$ for $i \neq j$.

Here are the MKL weights:
\begin{figure}[!ht]
  \centering
    \includegraphics[scale=.3]{mkl_weight_boxplot_christmas_star_DNA.png}
\end{figure}

And the power:
\begin{figure}[!ht]
  \centering
    \includegraphics[scale=.3]{power_christmas_star_DNA_facet4.png}
\end{figure}

The effect of $C$:
\begin{figure}[!ht]
  \centering
      \includegraphics[scale=.1]{tstat_on_c.png}
\end{figure}

$C$ clearly has an effect, especially if you get it very wrong (>.8).
I'm a little disappointed in the performance of MKL but pleased that
it does pick out the right structure in the data.  I'd like to say
that if you know the structure of the data a priori and use that in
the kernel, you will obviously get the best performance.  It seems
like you give up a lot of performance using MKL (maybe too much to
justify its convenience), only doing better than the worst choices of
kernels given.

MKL can pick out the structure of the data:
\begin{figure}[!ht]
  \centering
      \includegraphics[scale=.5]{MKL1.png}
\end{figure}

\section{Wine Example}
TODO
