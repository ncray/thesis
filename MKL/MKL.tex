\graphicspath{{./MKL/img/}}
\chapter{Multiple Kernels}
\label{C:MKL}
In this chapter we introduce a framework for two-sample testing
of heterogeneous data via multiple kernel learning (MKL).

\section{Introduction}
Given a set of kernels, it is possible to combine them in order to produce new
kernels.  This is a starting point for heterogeneous data analysis: we
can define a kernel $K_i$ for each data domain and develop a kernel $K$ that operates
on the union of the domains.  We typically shall produce a parametrized
family of kernels $K_{{\boldsymbol \theta}}$ and seek the ``best'' choice of
parameters ${\boldsymbol \theta}$.

For example, the class of kernel functions on $\mathcal{X}$ is closed under pointwise products
(also known as Schur products) of kernels,
\begin{equation*}
  K(x,x') = (K_1K_2)(x,x')=K_1(x,x')K_2(x,x'),
\end{equation*}
tensor products of kernels,
\begin{equation*}
 K(x, x') = (K_1 \otimes  K_2)(x_1,x_2,x_1',x_2')=K_1(x_1,x_1')K_2(x_2,x_2'),
\end{equation*}
and conic combinations of kernels,
\begin{equation*}
  K_{{\boldsymbol \theta}}(x, x') = (\theta_1 K_1 + \ldots + \theta_m K_m)(x,x')=
  \theta_1 K_1(x,x') + \ldots + \theta_m K_m(x,x').
\end{equation*}

An unweighted sum of kernels is equivalent to concatenating the individual
feature spaces.

\section{Multiple Kernel Learning}
In a landmark paper, Lanckriet et al.\ \cite{lanckriet2004learning} showed
that for various SVM objective functions, the problem of finding the optimal
conic combination of kernels could be posed as a convex optimization problem.
Although the initial approach involved solving a computationally expensive
semidefinite program, this sparked a flurry of research on similar convex
approaches to MKL.

Kloft et al.\ \cite{kloft2011lp} conceived a general $\ell_p$-norm approach
to MKL, unifying many special cases and further proposed a highly efficient
algorithm.  This can be seen as generalizing Problem~(\ref{eq:hinge-svm})
of Chapter~\ref{C:friedman-test}.

Let $L: \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ be a loss function,
$\Omega : \mathcal{H} \to \mathbb{R}$ be a regularizer, and $\lambda > 0$
be a tradeoff parameter.

Kloft et al.\ consider linear models of the form
\begin{equation*}
  h_{\tilde{w},b,\theta}(x) = \sum_{i=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m (x) \rangle_{\mathcal{H}_m} + b
  = \langle \tilde{w}, \phi_{\theta}(x) \rangle_{\mathcal{H}} + b,
\end{equation*}
where $\tilde{w} = [\tilde{w}_1^T,\ldots,\tilde{w}_m^T]^T$ and
$\phi_{\theta} = \sqrt{\theta_1} \phi_1 \times \ldots \times \sqrt{\theta_m} \phi_m$.

The regularized risk minimization problem is the following:
\begin{equation}
  \label{eq:mkl-rrm-1}
  \min_{\tilde{w}, b, \theta : \theta \succeq 0} \frac{1}{n} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{\lambda}{2}\sum_{m=1}^M \norm{\tilde{w}_m}_{\mathcal{H}_m}^2 +
  \tilde{\mu} \tilde{\Omega}(\theta),
\end{equation}
for $\tilde{\mu} > 0$.

Problem~(\ref{eq:mkl-rrm-1}) is not convex but can be transformed into a convex
problem via the substitution
\begin{equation*}
  w_m \leftarrow \sqrt{\theta_m} \tilde{w}_m.
\end{equation*}

Decoupling the regularization parameter from the sample size by letting
$\tilde{C} = \frac{1}{n \lambda}$ and $\mu \leftarrow \frac{\tilde{\mu}}{\lambda}$,
and using convex regularizers of the form $\tilde{\Omega}(\theta) = \norm{\theta}_p^2$,
we get
\begin{equation}
  \label{eq:mkl-rrm-2}
  \min_{w, b, \theta : \theta \succeq 0} \tilde{C} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} +
  \mu\norm{\theta}_p^2,
\end{equation}
where $\frac{t}{0} = 0$ if $t=0$ and $\infty$ otherwise.

Kloft et al.\ prove that the Tikhonov-regularized Problem (\ref{eq:mkl-rrm-2})
with two parameters is in fact equivalent to the following Ivanov-regularized
formulation with one regularization parameter, $C$:
\begin{equation}
\label{eq:mkl-ivanov}
\begin{aligned}
& \underset{w, b, \theta : \theta \succeq 0}{\text{minimize}}
& & C \sum_{i=1}^n L \left (\sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
\right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} \\
& \text{subject to}
& & \norm{\theta}_p^2 \leq 1.
\end{aligned}
\end{equation}

We use Problem~(\ref{eq:mkl-ivanov}) as implemented in SHOGUN \cite{sonnenburg2010shogun}.

\section{Simulation}
We generate heterogeneous data triplets $(\mathbf{x}_i, s_i, y_i)$, where
$\mathbf{x}_i \in \mathbb{R}^2$, $s_i \in \mathcal{S}$, the set of finite
length sequences of \{A, C, T, G\}, and $y_i \in \{-1, 1\}$.
$\mathbf{x}_i$ are drawn independently from the star example from \cite{sonnenburg2006large}.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/star.tex}
    }
  \end{center}
\caption{Star Distribution: radius 4 versus radius > 4.}
\label{fig:star}
\end{figure}

The star has a single radius parameter, $r$.  $p_{\mathbb{R}^2}$ takes $r = 4$,
and $q_{\mathbb{R}^2}$ takes $r > 4$.

Each $s_i$ has independent length $N \sim \text{Pois}(100)$ and is generated via
a Markov chain.  The start of the sequence is drawn from the stationary distribution
$[.25, .25, .25, .25]$, and the following $N-1$ elements are drawn according to the transition
probabilities
\begin{equation*}
  M(p^{\star}) = \bordermatrix{
    ~ & A & C & T & G \cr
    A & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
    C & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} \cr
    T & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} \cr
    G & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
  }
\end{equation*}

$p_{\mathcal{S}}$ takes $p_{\mathcal{S}}^{\star} = .25$ and
$q_{\mathcal{S}}$ takes $p_{\mathcal{S}}^{\star} > .25$.  Note that
$p_{\mathcal{S}}$ and $q_{\mathcal{S}}$ generate similar numbers of
$1$-mers, but $q_{\mathcal{S}}$ can generate more AC, CT, TG, GA
$2$-mers.

\section{Kernel Normalization}
Kernel normalization can have a large effect on the performance of kernel- and MKL-based
algorithms.  In the regularized regression setting, it is common to standardize variables to have
mean zero and unit variance so that the results are invariant to differences in
the unit of measurement.  Normalization plays a similar role in MKL.  For instance,
the Gaussian RBF kernel has $K(x, x) = 1$.  When considering long strings, it is
common for the $k$-spectrum kernel to take on large values.

We pre-process all kernels used in MKL by ensuring that the vectors in the feature
space lie on the unit hypersphere:
\begin{equation*}
  K_i(x, x') \leftarrow \frac{K_i(x, x')}{\sqrt{K_i(x, x)}\sqrt{K_i(x', x')}}
\end{equation*}

\section{MKL Weights}
Typically, two-sample tests provide a single bit of information: accept or reject the null hypothesis
that the two samples arose from the same distribution.
The MKL algorithm--or any other learning procedure that generates interpretable
weights--provides useful ancillary information in the kernel weights.  Here we
investigate the degree to which MKL is able to learn the structure of the data
and identify the data domains with the highest signal in discrimininating between
the two samples.

We include 4 Gaussian RBF kernels with widths $\{3.2, 10, 31.6, 100\}$ and $1$- and
$2$-spectrum kernels.  By design, only the $2$-spectrum kernel should be discriminatory
on the DNA string data.  We perform both $1$- and $2$- norm MKL.

In Figure~\ref{fig:mkl_weights1}, we fix the radius of the inner star to be $r = 4$
and the outer star to be $r = 4.5$.  We draw 200 samples from each distribution and
vary the signal on the DNA string data by letting $p^{\star}$ vary in $\{.25, .3, .35, .4\}$.
We fix the regularization parameter $C = 0.1$, and perform 100 permutations of the labels.
The unpermuted weights are shown as red points, and the permuted labels give rise to
boxplots of weights for both the $1$-norm and $2$-norm MKL.

We see that as we increase the difference between $p_{\mathcal{S}}$ and $q_{\mathcal{S}}$,
more weight is being assigned to the $2$-spectrum kernel.  $1$-norm MKL yields sparse weight
vectors.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_weights_star_dna.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the $2$-spectrum kernel as the DNA signal is increased.}
\label{fig:mkl_weights1}
\end{figure}

In Figure~\ref{fig:mkl_weights2} we vary the radius of the outer star in $\{4, 7, 10, 13, 16\}$,
while fixing the radius of the inner star to be $4$ and the transition probability $p^{\star} = .3$.
We see the dominant weight for the unpermuted case shift to higher-width kernels as we increase
the radius of the outer star.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_weights_star_dna2.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the higher-width RBF kernels as we increase the distance between the two stars.}
\label{fig:mkl_weights2}
\end{figure}

\section{Power}
Given that we have successfully learned the structure of the data, we now
investigate the statistical power of these methods.  For each simulation, we take
$50$ samples from each distribution and fix $C = 1$.

In Figure~\ref{fig:mkl_power} we vary the radius of the outer star in the rows of the plot,
taking values in $\{4, 4.3, 4.6\}$, and the $x$-axis sees the transition probability
take values in $\{.25, .3, .35, .4, .45\}$.  We compare the power of three RBF kernels
with widths 5, 10, and 100, the $1$- and $2$-spectrum kernels, and $1$- and $2$-norm MKL
taking convex combinations of these five kernels.

We see that for a radius of 4 and transition probability of .25, $p=q$, and the power
is equal to the level of the test, $\alpha = .05$.  The MKL-based tests consistently
perform second best, just behind the top-performing kernel in each setting.  It appears
that there is a minor penalty in performance to be paid for selecting the kernel
weights versus a priori placing all weight on the best kernel for the job.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/dna_star_power.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the higher-width RBF kernels as we increase the distance between the two stars.}
\label{fig:mkl_power}
\end{figure}

\section{Null Distribution}
Permutation-based tests exact an onerous computational burden, requiring computation
proportional to the number of permutations in order to conduct meaningful statistical
inference.  Thus, distributional approximations to these discrete, permutation null
distributions are of great interest.

Here we compare the null distributions over 2000 permutations of the labels in each scenario,
adjusting the regularization parameter $C \in \{.1, 1, 10\}$.  We report $p$-values
from the Anderson--Darling test for normality in Figure~\ref{fig:mkl_null}.  Except for
the situation with the highest emphasis on the loss function ($C=10$), the
permutation null samples are consistent with the standard normal distribution for all kernels
and MKL statistics.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_null_dist.tex}
    }
  \end{center}
\caption{Except for $C=10$, permutation null samples are consistent with the standard normal distribution.}
\label{fig:mkl_null}
\end{figure}