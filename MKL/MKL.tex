\graphicspath{{./MKL/img/}}
\chapter{Multiple Kernels}
\label{C:MKL}
In this chapter we introduce a framework for two-sample testing
of heterogeneous data via multiple kernel learning (MKL).

\section{Introduction}
Given a set of kernels, it is possible to combine them in order to produce new
kernels.  This is a starting point for heterogeneous data analysis: we
can define a kernel $K_i$ for each data domain and develop a kernel $K$ that operates
on the union of the domains.  We typically shall produce a parametrized
family of kernels $K_{{\boldsymbol \theta}}$ and seek the ``best'' choice of
parameters ${\boldsymbol \theta}$.

For example, the class of kernel functions on $\mathcal{X}$ is closed under pointwise products
(also known as Schur products) of kernels,
\begin{equation*}
  K(x,x') = (K_1K_2)(x,x')=K_1(x,x')K_2(x,x'),
\end{equation*}
tensor products of kernels,
\begin{equation*}
 K(x, x') = (K_1 \otimes  K_2)(x_1,x_2,x_1',x_2')=K_1(x_1,x_1')K_2(x_2,x_2'),
\end{equation*}
and conic combinations of kernels,
\begin{equation*}
  K_{{\boldsymbol \theta}}(x, x') = (\theta_1 K_1 + \ldots + \theta_m K_m)(x,x')=
  \theta_1 K_1(x,x') + \ldots + \theta_m K_m(x,x').
\end{equation*}

An unweighted sum of kernels is equivalent to concatenating the individual
feature spaces.

\section{Multiple Kernel Learning}
In a landmark paper, Lanckriet et al.\ \cite{lanckriet2004learning} showed
that for various SVM objective functions, the problem of finding the optimal
conic combination of kernels could be posed as a convex optimization problem.
Although the initial approach involved solving a computationally expensive
semidefinite program, this sparked a flurry of research on similar convex
approaches to MKL.

Kloft et al.\ \cite{kloft2011lp} conceived a general $\ell_p$-norm approach
to MKL, unifying many special cases and further proposed a highly efficient
algorithm.  This can be seen as generalizing Problem~(\ref{eq:hinge-svm})
of Chapter~\ref{C:friedman-test}.

Let $L: \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ be a loss function,
$\Omega : \mathcal{H} \to \mathbb{R}$ be a regularizer, and $\lambda > 0$
be a tradeoff parameter.

Kloft et al.\ consider linear models of the form
\begin{equation*}
  h_{\tilde{w},b,\theta}(x) = \sum_{i=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m (x) \rangle_{\mathcal{H}_m} + b
  = \langle \tilde{w}, \phi_{\theta}(x) \rangle_{\mathcal{H}} + b,
\end{equation*}
where $\tilde{w} = [\tilde{w}_1^T,\ldots,\tilde{w}_m^T]^T$ and
$\phi_{\theta} = \sqrt{\theta_1} \phi_1 \times \ldots \times \sqrt{\theta_m} \phi_m$.

The regularized risk minimization problem is the following:
\begin{equation}
  \label{eq:mkl-rrm-1}
  \min_{\tilde{w}, b, \theta : \theta \succeq 0} \frac{1}{n} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \sqrt{\theta_m} \langle \tilde{w}_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{\lambda}{2}\sum_{m=1}^M \norm{\tilde{w}_m}_{\mathcal{H}_m}^2 +
  \tilde{\mu} \tilde{\Omega}(\theta),
\end{equation}
for $\tilde{\mu} > 0$.

Problem~(\ref{eq:mkl-rrm-1}) is not convex but can be transformed into a convex
problem via the substitution
\begin{equation*}
  w_m \leftarrow \sqrt{\theta_m} \tilde{w}_m.
\end{equation*}

Decoupling the regularization parameter from the sample size by letting
$\tilde{C} = \frac{1}{n \lambda}$ and $\mu \leftarrow \frac{\tilde{\mu}}{\lambda}$,
and using convex regularizers of the form $\tilde{\Omega}(\theta) = \norm{\theta}_p^2$,
we get
\begin{equation}
  \label{eq:mkl-rrm-2}
  \min_{w, b, \theta : \theta \succeq 0} \tilde{C} \sum_{i=1}^n L \left (
  \sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
  \right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} +
  \mu\norm{\theta}_p^2,
\end{equation}
where $\frac{t}{0} = 0$ if $t=0$ and $\infty$ otherwise.

Kloft et al.\ prove that the Tikhonov-regularized Problem (\ref{eq:mkl-rrm-2})
with two parameters is in fact equivalent to the following Ivanov-regularized
formulation with one regularization parameter, $C$:
\begin{equation}
\label{eq:mkl-ivanov}
\begin{aligned}
& \underset{w, b, \theta : \theta \succeq 0}{\text{minimize}}
& & C \sum_{i=1}^n L \left (\sum_{m=1}^M \langle w_m, \phi_m(x_i) \rangle_{\mathcal{H}_m} + b, y_i
\right) + \frac{1}{2} \sum_{m=1}^M \frac{\norm{w_m}_{\mathcal{H}_m}^2}{\theta_m} \\
& \text{subject to}
& & \norm{\theta}_p^2 \leq 1.
\end{aligned}
\end{equation}

We use Problem~(\ref{eq:mkl-ivanov}) as implemented in SHOGUN \cite{sonnenburg2010shogun}.

\section{Simulation}
We generate heterogeneous data triplets $(\mathbf{x}_i, s_i, y_i)$, where
$\mathbf{x}_i \in \mathbb{R}^2$, $s_i \in \mathcal{S}$, the set of finite
length sequences of \{A, C, T, G\}, and $y_i \in \{-1, 1\}$.
$\mathbf{x}_i$ are drawn independently from the star example from \cite{sonnenburg2006large}.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/star.tex}
    }
  \end{center}
\caption{Star Distribution: radius 4 versus radius > 4.}
\label{fig:star}
\end{figure}

The star has a single radius parameter, $r$.  $p_{\mathbb{R}^2}$ takes $r = 4$,
and $q_{\mathbb{R}^2}$ takes $r > 4$.

Each $s_i$ has independent length $N \sim \text{Pois}(100)$ and is generated via
a Markov chain.  The start of the sequence is drawn from the stationary distribution
$[.25, .25, .25, .25]$, and the following $N-1$ elements are drawn according to the transition
probabilities
\begin{equation*}
  M(p^{\star}) = \bordermatrix{
    ~ & A & C & T & G \cr
    A & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
    C & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} & \frac{1-p^{\star}}{3} \cr
    T & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & p^{\star} \cr
    G & p^{\star} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} & \frac{1-p^{\star}}{3} \cr
  }
\end{equation*}

$p_{\mathcal{S}}$ takes $p_{\mathcal{S}}^{\star} = .25$ and
$q_{\mathcal{S}}$ takes $p_{\mathcal{S}}^{\star} > .25$.  Note that
$p_{\mathcal{S}}$ and $q_{\mathcal{S}}$ generate similar numbers of
$1$-mers, but $q_{\mathcal{S}}$ can generate more AC, CT, TG, GA
$2$-mers.

\section{Kernel Normalization}
Kernel normalization can have a large effect on the performance of kernel- and MKL-based
algorithms.  In the regularized regression setting, it is common to standardize variables to have
mean zero and unit variance so that the results are invariant to differences in
the unit of measurement.  Normalization plays a similar role in MKL.  For instance,
the Gaussian RBF kernel has $K(x, x) = 1$.  When considering long strings, it is
common for the $k$-spectrum kernel to take on large values.

We pre-process all kernels used in MKL by ensuring that the vectors in the feature
space lie on the unit hypersphere:
\begin{equation*}
  K_i(x, x') \leftarrow \frac{K_i(x, x')}{\sqrt{K_i(x, x)}\sqrt{K_i(x', x')}}
\end{equation*}

\section{MKL Weights}
Typically, two-sample tests provide a single bit of information: accept or reject the null hypothesis
that the two samples arose from the same distribution.
The MKL algorithm--or any other learning procedure that generates interpretable
weights--provides useful ancillary information in the kernel weights.  Here we
investigate the degree to which MKL is able to learn the structure of the data
and identify the data domains with the highest signal in discrimininating between
the two samples.

We include 4 Gaussian RBF kernels with widths $\{3.2, 10, 31.6, 100\}$ and $1$- and
$2$-spectrum kernels.  By design, only the $2$-spectrum kernel should be discriminatory
on the DNA string data.  We perform both $1$- and $2$- norm MKL.

In Figure~\ref{fig:mkl_weights1}, we fix the radius of the inner star to be $r = 4$
and the outer star to be $r = 4.5$.  We draw 200 samples from each distribution and
vary the signal on the DNA string data by letting $p^{\star}$ vary in $\{.25, .3, .35, .4\}$.
We fix the regularization parameter $C = 0.1$, and perform 100 permutations of the labels.
The unpermuted weights are shown as red points, and the permuted labels give rise to
boxplots of weights for both the $1$-norm and $2$-norm MKL.

We see that as we increase the difference between $p_{\mathcal{S}}$ and $q_{\mathcal{S}}$,
more weight is being assigned to the $2$-spectrum kernel.  $1$-norm MKL yields sparse weight
vectors.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_weights_star_dna.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the $2$-spectrum kernel as the DNA signal is increased.}
\label{fig:mkl_weights1}
\end{figure}

In Figure~\ref{fig:mkl_weights2} we vary the radius of the outer star in $\{4, 7, 10, 13, 16\}$,
while fixing the radius of the inner star to be $4$ and the transition probability $p^{\star} = .3$.
We see the dominant weight for the unpermuted case shift to higher-width kernels as we increase
the radius of the outer star.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_weights_star_dna2.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the higher-width RBF kernels as we increase the distance between the two stars.}
\label{fig:mkl_weights2}
\end{figure}

\section{Power}
Given that we have successfully learned the structure of the data, we now
investigate the statistical power of these methods.  For each simulation, we take
$50$ samples from each distribution and fix $C = 1$.

In Figure~\ref{fig:mkl_power} we vary the radius of the outer star in the rows of the plot,
taking values in $\{4, 4.3, 4.6\}$, and the $x$-axis sees the transition probability
take values in $\{.25, .3, .35, .4, .45\}$.  We compare the power of three RBF kernels
with widths 5, 10, and 100, the $1$- and $2$-spectrum kernels, and $1$- and $2$-norm MKL
taking convex combinations of these five kernels.

We see that for a radius of 4 and transition probability of .25, $p=q$, and the power
is equal to the level of the test, $\alpha = .05$.  The MKL-based tests consistently
perform second best, just behind the top-performing kernel in each setting.  It appears
that there is a minor penalty in performance to be paid for selecting the kernel
weights versus a priori placing all weight on the best kernel for the job.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/dna_star_power.tex}
    }
  \end{center}
\caption{The MKL weights in the 1- (upper row) and 2-norm (lower row) cases shift progressively more to
  the higher-width RBF kernels as we increase the distance between the two stars.}
\label{fig:mkl_power}
\end{figure}

\section{Null Distribution}
Permutation-based tests exact an onerous computational burden, requiring computation
proportional to the number of permutations in order to conduct meaningful statistical
inference.  Thus, distributional approximations to these discrete, permutation null
distributions are of great interest.

Here we compare the null distributions over 2000 permutations of the labels in each scenario,
adjusting the regularization parameter $C \in \{.1, 1, 10\}$.  We report $p$-values
from the Anderson--Darling test for normality in Figure~\ref{fig:mkl_null}.  Except for
the situation with the highest emphasis on the loss function ($C=10$), the
permutation null samples are consistent with the standard normal distribution for all kernels
and MKL statistics.
\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_null_dist.tex}
    }
  \end{center}
\caption{Except for $C=10$, permutation null samples are consistent with the standard normal distribution.}
\label{fig:mkl_null}
\end{figure}

\section{Approximate Regression Condition}
Here we examine whether or not the approximate regression condition
holds in the MKL setting.
Taking the heterogeneous data example from before, we vary the
per-group sample size $n \in \{10, 50, 100\}$.  For each value of $T$,
we perform $30$ label swaps and compute the corresponding values $T'$
in Figure~\ref{fig:mkl_arc}.  The approximate regression condition
appears to hold in all settings, and this suggests that we may be able to derive
an error bound using the techniques from
Chapters~\ref{C:steins-method} and \ref{C:stein-proof}.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_arc.tex}
    }
  \end{center}
\caption{Different choices of kernels in the columns, and different
  sample sizes in the rows.}
\label{fig:mkl_arc}
\end{figure}

Given a large number of independent kernels, one worry is that our
learning method may overfit to limited data.  In order to explore this
possibility, we generate $X \sim \mathcal{MVN}_d({\bf 0}, {\boldsymbol I})$
data and associate to each entry $X_i$ a Gaussian RBF kernel of width
1.  Because the distribution is isotropic, we have $d$ independent
kernels.  We estimate the randomization distribution over 200
permutations with $Q$--$Q$ plots against the standard normal quantiles
and in addition report the Anderson--Darling $p$-values
in Figure~\ref{fig:overfit_mkl}.  It appears that the number of
independent kernels has a large effect on the normality of the
randomization distribution.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/overfit_mkl.tex}
    }
  \end{center}
\caption{The regularization parameter $C$ in the columns, and the
  number of Gaussian RBF kernels in the rows.}
\label{fig:overfit_mkl}
\end{figure}

We verify that the approximate regression condition fails to hold in
Figure~\ref{fig:mkl_arc_overfit}.  Given 100 independent kernels, the
remainder $R$ in Equation~(\ref{D:approx-stein-pair}) does not
appear to be a random variable of small and decreasing order.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/mkl_arc_overfit.tex}
    }
  \end{center}
\caption{Sample size in the rows, and number of independent kernels in
the columns.}
\label{fig:mkl_arc_overfit}
\end{figure}

\section{Wine Example}
In July 2012, we collected 78,443 descriptions of wine from the K\&L
Wine Merchants' website: \url{http://www.klwines.com/}.  Each
description includes characteristics about the wine such as its name,
price, varietal, critics' ratings and reviews, and winery location.

For example, here is the raw data for a 2002 Pinot Noir from Siduri Wines:
\begin{verbatim}
{"sku": "1011975",
  "info": {
    "Country:": "United States",
    "Specific Appellation:": "Sonoma County",
    "Sub-Region:": "California",
    "Varietal:": "Pinot Noir"
  },
  "title": "2002 Siduri \"Hirsch Vineyard\" Sonoma Pinot Noir
  (Previously $49.99)",
  "price": "39.99",
  "ratings": {
    "ST": "90"
  },
  "desc": "90 points Stephen Tanzer's International Wine Cellar:
  \"Full medium red. Sappy aromas of cherry, plum, raspberry and
  minerals, with complicating floral and forest floor notes. Sweet, fat
  and full of fruit; very smooth flavors of black cherry, licorice,
  mocha and bitter chocolate. Nicely balanced pinot, and showing well
  today.\" (May/Jun 04)"
}
\end{verbatim}

We'd like to know if our two-sample test can discriminate between
two different wine varietals on the basis of some of these features.
Moreover, we are also interested in understanding which are the most
influential features.  Because of the large number of samples of the
two varietals, we focus on the set of Pinot Noir and Chardonnay wines.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/wine_weights.tex}
    }
  \end{center}
\caption{Boxplots of 2-norm MLK weights, in decreasing order of
  highest variable weight for the corresponding model.}
\label{fig:wine_weights}
\end{figure}

We extract the vintage (year) of the wine into a new field and only consider
letters and spaces in the wine titles and descriptions.  After
removing examples without a Robert Parker rating, we have 879
Chardonnay and 1134 Pinot Noir examples remaining.  We use the
4-spectrum kernel for the string fields, title and description; the
Gaussian RBF kernel with width 1 for the numeric fields, price, Robert
Parker rating, and vintage; and the identity kernel for the
subregion.  Although simple, these kernel choices are also quite
reasonable: the numeric fields are all roughly on the same scale,
comparable with the RBF width.  The identity kernel takes on a value
of 1 only when the subregions agree entirely.  Thus, we lose
geographic information such as distances between subregions.

\begin{figure}
  \begin{center}
    \resizebox{14.0cm}{!}{
      \input{MKL/img/wine_power.tex}
    }
  \end{center}
\caption{Power on sample size: MKL can make effective use of
  additional information.}
\label{fig:wine_power}
\end{figure}

We train a 2-norm MKL with regularization parameter $C=.01$, initially
on all 6 kernels, with varying sample sizes in $\{50, 100, 150, 200,
250\}$, repeating each test 200 times in order to estimate the
distribution of the kernel weights and the statistical power of the
tests.  In Figure~\ref{fig:wine_weights} we report boxplots of the
kernel weights.  For the 6-kernel model on all the features, the dominant
weight lies on the 4-spectrum kernel associated with the name (title)
of the wine.  In Figure~\ref{fig:wine_power}, we see that this test
has the highest power.  We then remove the kernel with the highest
weight and fit a 5-kernel model on all of the features except for the
name of the wine.  We have ordered the boxplot x-axis in order of
the highest variable weight corresponding to the model under
consideration.  Thus, the title of the wine has the highest weight
for the 6-kernel model, the description of the wine has the highest
weight for the 5-kernel model, and so on.

The title of the wine sometimes contains information about the very
thing we're trying to predict -- the wine varietal.  Additionally,
many wineries specialize in particular varietals of wine.  For
example, Siduri only produces single vineyard Pinot Noirs from
vineyards located in California and Oregon.  Therefore, we should
expect that models incorporating this information should perform very
well.

Pinot Noir wines are typically lighter-colored reds that have notes of
red fruits such as cherries, strawberries, and raspberries.  The grape
requires a cooler climate for best results and is associated with the
Burgundy region of France and areas of California and Oregon.
Chardonnay, by contrast, is a white wine with flavors of buttery oak
and fruits such as apples and pears.  Chardonnay is also commonly
grown in Burgundy and California as well as Australia and New
Zealand.  The wine reviews use many adjectives and descriptors
associated with each varietal, in addition to sometimes including the
name of the varietal.  By constructing views of each data type through
kernels, we can pick up on such differences between the two varietals.

In Figure~\ref{fig:wine_power}, by repeatedly removing the
most-informative feature, we monotonically decrease the statistical
power of the test.  The final model including just the price and year
of the wine has very poor performance.  In fact, the distributions of
the weights of the two variables are very similar, indicating that
both features are equally poor predictors of the type of wine.  For
this wine example, MKL can effectively exploit additional data types
for higher discriminatory power.

\section{Future Work}
We hope to better understand the Friedman Test outside of the
univariate data and linear kernel setting.  That is, we would like to
characterize the kernels for which the Friedman Test generalizes the
permutation $t$-test and understand the degree to which this
relationship holds approximately given higher dimensional data and a
wide range of kernels.  We further hope to extend the theory of
Chapters~\ref{C:steins-method} and \ref{C:stein-proof} to prove rates
of convergence bounds in these general settings.  As seen in
Figures~\ref{fig:mkl_arc_overfit} and \ref{fig:overfit_mkl}, these bounds
would likely incorporate some terms that represent the learning
algorithm and numbers of independent kernels.

We can also potentially treat various kinds of missing data.
If we consider entire missing modalities (e.g. one sample is
missing some biometric reading), Poh et al.\ \cite{poh2010addressing}
develop the \emph{neutral point substitution} technique to allow
substitution of the missing modality with a new kernel that is
\emph{unbiased} with regard to the classification at hand.  This
allows for full use of both modalities that are present for all
samples as well as those that are present only for a subset of the
samples and effective utilization of all the data in the training set.
Panov et al.\ \cite{panov2011modified} modify the NPS method to allow
for missing modalities in the test set.