\chapter{Stein's method}
\label{C:steins-method}
In this chapter, we present an introduction to
Stein's method of exchangeable pairs, which we use to prove the core
theoretical result of this thesis: a rate of convergence bound on the
Kolmogorov distance between the randomization distribution of the $t$-statistic
and the standard normal distribution.

Due to similarities between this problem and Hoeffding's combinatorial central
limit theorem (HCCLT), we first review Stein's proof of the HCCLT via the method
of exchangeable pairs.

\section{Introduction}
\label{S:steins-method-introduction}
Stein's method provides a means
of bounding the distance between two probability distributions in a
given probability metric.  When applied with the normal distribution
as the target, this results in central-limit-type theorems.  Traditional
results of this type typically make use of the complex-valued characteristic
function.  For an account of the early history and many variations of the
Central Limit Theorem, see Le Cam \cite{cam1986central}.
Stein introduced a groundbreaking approach with a real characterizing
equation in order more effectively tame various forms of dependence in
\cite{stein1972bound}.

He soon refined these initial ideas into his monograph
\cite{stein1986approximate}, which includes the Poisson approximation
work of Chen \cite{chen1975poisson}, random graph results of Barbour
and Eagleson \cite{barbour1985multiple}, and binomial approximation of
Diaconis \cite{diaconis1977distribution}.  Several flavors of Stein's
method (e.g. the method of exchangeable pairs) proceed via auxiliary
randomization.  Other compilations with many instructive applications
include those of Diaconis and Holmes \cite{diaconis2004stein}, Barbour
and Chen \cite{barbour2005introduction, barbour2005stein}, and
Chen, Goldstein, and Shao \cite{chen2010normal}.

It will be instructive to follow the proof of the HCCLT
because our proof proceeds in a similar fashion but with the following
generalizations: an approximate contraction property, less
cancellation of terms due to separate estimation of various
denominators, and non-unit variance of the random variable of interest.
In Appendix~\ref{A:steins-method-app}, we reproduce
Stein's \cite{stein1986approximate} proof of the HCCLT.
We also refer to auxiliary results in Appendix~\ref{A:auxiliary-app}.

\section{Stein's Theorem}
Theorem~\ref{T:stein-theorem} bounds the Kolmogorov distance between the
distribution of the random variable $W$ and the standard normal distribution
in terms of functions of the difference of the exchangeable pair $(W, W')$.  It is
applied to the situation where $W$ is the sum of the random diagonal of a matrix
to prove the HCCLT.  Chen et al.\ \cite{chen2010normal}
generalize Theorem~\ref{T:stein-theorem} to allow
for situations in which the regression condition does not hold exactly, and
we later in addition relax the assumption that the $W$ has unit variance.
\begin{theorem}[Stein]\label{T:stein-theorem}
  If $W$, $W'$ are mean 0, exchangeable random variables with variance 1
  satisfying the exact regression condition
  \begin{equation*}
    \E[W'-W|W] = -\lambda W
  \end{equation*}
  for some $\lambda \in (0,1)$, then
  \begin{align*}
    \sup_{w \in \mathbb{R}} |P(W \leq w) - \Phi(w)| &\leq
    2\sqrt{\E \left [1 - \frac{1}{2 \lambda}E[(W'-W)^2|W] \right ]} +
    (2 \pi)^{-1/4}\sqrt{\frac{1}{\lambda}\E |W'-W|^3} \\
    &\leq 2 \sqrt{\var (\E [(W'-W)^2|W])} +
    (2 \pi)^{-1/4}\sqrt{\frac{1}{\lambda}\E |W'-W|^3}.
  \end{align*}
\end{theorem}

\section{Hoeffding's Combinatorial CLT}
Stein's proof of the HCCLT relies on an application of Theorem~\ref{T:stein-theorem}.
\begin{theorem}[Hoeffding's Combinatorial Central Limit Theorem]\label{T:HCCLT}
  Let $\{a_{ij}\}_{i,j}$ be an $n \times n$ matrix of real-valued
  entries that is row- and column-centered and scaled such that the sums
  of the squares of its elements equals $n-1$:
  \begin{align*}
    \sum_{j=1}^n a_{ij} &= 0 \\
    \sum_{i=1}^n a_{ij} &= 0 \\
    \sum_{i=1, j=1}^n a_{ij}^2 &= n-1
  \end{align*}
  Let $\Pi$ be a random permutation of $\{1, \ldots, n\}$ drawn
  uniformly at random from the set of all permutations:
  \begin{equation*}
    P(\Pi = \pi) = \frac{1}{n!}.
  \end{equation*}
  Define
  \begin{equation*}
    W = \sum_{i=1}^n a_{i\Pi(i)}
  \end{equation*}
  to be the sum of a random diagonal.  Then
  \begin{equation*}
    |P(W \leq w) - \Phi(w)| \leq
    \frac{C}{\sqrt{n}} \left [
      \sqrt{\sum_{i, j = 1}^{n} a_{ij}^4} +
      \sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}
    \right ].
  \end{equation*}
\end{theorem}

Originally proved by Hoeffding using the method of moments
\cite{hoeffding1951combinatorial}, Theorem~\ref{T:HCCLT} generalizes
the non-parametric measure of rank correlation known as Spearman's
footrule \cite{spearman1904proof} \cite{salama1990note}
\begin{equation*}
  D(\pi, \sigma) = \sum_{i=1}^n |\pi(i) - \sigma(i)|,
\end{equation*}
where $\pi$ and $\sigma$ are elements of $S_n$, the set of
permutations of $n$ letters.  Considering $D$ as a metric on $S_n$,
Diaconis and Graham \cite{diaconis1977spearman} discuss its
relationship with other common non-parametric measures and derive
various asymptotic results.  In fact, Spearman's footrule has found
contemporary applications such as in ranking search queries and microarray
experiments \cite{sens2011spearman}, where consistency of the top
$k$ out of $n$ objects is desired.

Theorem~\ref{T:HCCLT} was extended by Motoo \cite{motoo1956hoeffding} and
also generalizes the work on permutation-based
tests of Wald and Wolfowitz \cite{wald1944statistical} and Noether
\cite{noether1949theorem}.  These results build further on the problem of
devising exact tests of significance when the underlying probability
distribution is unknown, originating from Fisher
\cite{fisher1935design, fisher1970statistical}.

Given fixed data $a_{ij}$, the HCCLT provides a bound in terms of a
universal constant $C$, a sample-size-dependent term
$\frac{1}{\sqrt{n}}$, and a function of the data $\sqrt{\sum_{i, j =
1}^{n} a_{ij}^4} + \sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}$.  Thus,
given $C$, we can calculate an explicit bound on the Kolmogorov
distance.  Consider a sequence of matrices, $a_{ij}^{(n)}$, of
dimension $n \times n$.  In order to achieve a $\mathcal{O}(n^{-1/2})$
rate of convergence, we require the function of the data to be
bounded.  However, this will not typically be the case.

Bolthausen \cite{bolthausen1984estimate} was able to prove the following result:
\begin{theorem}[Bolthausen]
  Under the conditions of Theorem~\ref{T:HCCLT}, there is an absolute constant
  $K > 0$ such that
  \begin{equation*}
    |P(W \leq w) - \Phi(w)| \leq K \frac{\sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}}{n}.
  \end{equation*}
\end{theorem}

Then, given the sequence of matrices $a_{ij}^{(n)}$, the theorem
yields a convergence rate of $\mathcal{O}(n^{-1/2})$ as long as
$\sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3} / \sqrt{n}$ remains bounded.

von Bahr \cite{bahr1976remainder} and Ho and Chen \cite{ho1978l_p}
also obtained the same rate under some boundedness conditions
on random-valued entries of the matrix.  For example, von
Bahr was able to obtain the following result:
\begin{theorem}[von Bahr]
  Let $\{X_{ij}\}_{i, j = 1}^n$ be a square matrix of random variables with
  independent row vectors.
  With $\mu(i, j) = \E X_{ij}$, define
  \begin{equation*}
    \bar{\mu}(i, \cdot) = n^{-1} \sum_j \mu(i, j), \quad
    \bar{\mu}(\cdot, j) = n^{-1} \sum_i \mu(i, j), \quad
    \bar{\mu}(\cdot, \cdot) = n^{-2} \sum_i \sum_j \mu(i, j). \quad
  \end{equation*}
  Set $Y_{ij} = X_{ij} - \bar{\mu}(i, \cdot) - \bar{\mu}(\cdot, j) + \bar{\mu}(\cdot, \cdot)$,
  $\tau_{ij}^2 = \E Y_{ij}^2$, and $\tau^2 = n^{-2} \sum_{i, j} \tau_{ij}^2$.

  Let $\Pi$ be a uniformly at random permutation, independent of $X_{ij}$, and
  $W = \sum_{i=1}^n X_{i\Pi(i)}$ be the sum of a random diagonal.
  Then there exists an absolute constant $C$ such that for all $x \in \mathbb{R}$,
  \begin{equation*}
    \left |P \left (\frac{W - \mu_W}{\sigma_W} \leq w \right ) - \Phi(w) \right |
    \leq \frac{C}{\sqrt{n}} \gamma,
  \end{equation*}
  where $\mu_W$ denotes the mean of $W$, $\sigma_W$ its standard deviation,
  $\Phi(w)$ the standard normal CDF, and
  \begin{equation*}
    \gamma = \max \left [
      \max_{i, j} \E |Y_{ij}| / \tau,
      \max_i \sum_j \E Y_{ij}^2 / n \tau^2,
      \max_j \sum_i \E Y_{ij}^2 / n \tau^2,
      \max_{i, j} \sum_{i, j} \E |Y_{ij}|^3 / n^2 \tau^3
    \right ]
  \end{equation*}
\end{theorem}

Now, we return to generalizing Theorem~\ref{T:stein-theorem}.

% Sourav got to me and says he doesn't see any problems with your proofs
% but says that for the written thesis you have to do a better on the
% bibliography section of the Berry Esseen bounds for self normalized
% sums which apparently you have not covered carefully enough.

% \section{Exchangeable Pairs}
% TODO: Add a lot of development for exchangeable pairs.  For now,
% focusing on generalizing the theorems in ``Normal Approximation by
% Stein's Method.'' \cite{chen2010normal}

% Theorem 5.5 in ``Normal Approximation by Stein's Method'' concerns
% variance 1 exchangeable random variables.  Our setting has the
% variance tending to 1, so we first prove a slight generalization of
% the theorem.  Large parts of the proof are copied verbatim from the
% book.
\section{Generalized Stein's Theorems}
Here, we treat the situation where the regression condition fails to hold exactly.
Chen et al.\ \cite{chen2010normal} serves as an excellent reference for results
of this type.

\begin{definition}[Approximate Stein Pair]
  Let $(W, W')$ be an exchangeable pair.  If the pair satisfies the ``approximate linear
  regression condition''
  \begin{equation}
    \label{D:approx-stein-pair}
    \E [W - W' | W] = \lambda (W - R),
  \end{equation}
  where $R$ is a variable of small order and $\lambda \in (0, 1)$, then we call $(W, W')$ an
  approximate Stein pair.
\end{definition}

Here we generalize Lemma 5.1 from \cite{chen2010normal} to the setting of non-unit variance:
\begin{theorem}
  \label{L:stein-difference-second-moment-generalization-bound}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance $\E W^2$
  satisfying
  \begin{equation*}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$, then for any
  $z \in \mathbb{R}$ and $a > 0$,
  \begin{equation*}
    \E [(W'-W)^2 \mathbf{1}_{\{-a \leq W' - W \leq 0\}} \mathbf{1}_{\{z-a \leq W \leq z\}}] \leq
    3a \lambda (\sqrt{\E W^2} + \E |R|)
  \end{equation*}
  and
  \begin{equation*}
    \E [(W'-W)^2 \mathbf{1}_{\{0 \leq W' - W \leq a\}} \mathbf{1}_{\{z-a \leq W \leq z\}}] \leq
        3a \lambda (\sqrt{\E W^2} + \E |R|).
  \end{equation*}
\end{theorem}

The following theorem will allow us to prove a $\mathcal{O}(n^{-1/4})$ rate under
mild conditions.  Generalization of Theorem 5.5 from \cite{chen2010normal}:
\begin{theorem}
  \label{T:main}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance
  $\E W^2$
  satisfying
  \begin{equation*}
    \label{eq:13}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$, then
  \begin{equation*}
    \begin{split}
      \sup_{z \in \mathbb{R}} |P(W \leq z) - \Phi(z)|
      &\leq (2\pi)^{-1/4} \sqrt{\frac{\E |W'-W|^3}{\lambda}}
      + \frac{1}{2\lambda} \sqrt{\var (\E [(W'-W)^2|W])} \\
      &\quad + |\E W^2 - 1| + \E |WR| + \E |R|
    \end{split}
  \end{equation*}
\end{theorem}

The following result will let us achieve a rate of $\mathcal{O}(n^{-1/2})$ subject to
the difference $|W'-W|$ being bounded.  This condition has been similarly used by
Rinott and Rotar \cite{rinott1997coupling} and Shao and Su \cite{shao2006berry}.
It generalizes part of Theorem 5.3 from \cite{chen2010normal}:
\begin{theorem}
  \label{T:better-rate}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance
  $\E W^2$
  satisfying
  \begin{equation*}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$ and $|W'-W| \leq \delta$, then
  \begin{equation*}
    \begin{split}
      \sup_{z \in \mathbb{R}} |P(W \leq z) - \Phi(z)|
      &\leq \frac{.41 \delta^3}{\lambda} + 3 \delta (\sqrt{\E W^2} + \E |R|)
      + \frac{1}{2\lambda} \sqrt{\var (\E [(W'-W)^2|W])} \\
      &\quad + |\E W^2 - 1| + \E |WR| + \E |R|
    \end{split}
  \end{equation*}
\end{theorem}
