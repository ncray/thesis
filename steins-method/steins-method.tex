\chapter{Stein's method}
\label{C:steins-method}
In this chapter, we present an introduction to
Stein's method of exchangeable pairs, which we use to prove the core
theoretical result of this thesis: a rate of convergence bound on the
Kolmogorov distance between the randomization distribution of the $t$-statistic
and the standard normal distribution.

Due to similarities between this problem and Hoeffding's combinatorial central
limit theorem (HCCLT), we first review Stein's proof of the HCCLT via the method
of exchangeable pairs.


\section{Introduction}
\label{S:steins-method-introduction} Stein's method provides a means
of bounding the distance between two probability distributions in a
given probability metric.  When applied with the normal distribution
as the target, this results in central-limit-type theorems.  Several
flavors of Stein's method (e.g. the method of exchangeable pairs)
proceed via auxiliary randomization.  In Appendix~\ref{A:steins-method-app},
we reproduce Stein's \cite{stein1986approximate} proof of the HCCLT.

It will be instructive to follow the proof of the HCCLT
because our proof proceeds in a similar fashion but with the following
generalizations: an approximate contraction property, less
cancellation of terms due to separate estimation of various
denominators, and non-unit variance of the random variable of interest.
We also refer to auxiliary results in Appendix~\ref{A:auxiliary-app}.

\section{Stein's Theorem}
Theorem~\ref{T:stein-theorem} bounds the Kolmogorov distance between the
distribution of the random variable $W$ and the standard normal distribution
in terms of functions of the difference of the exchangeable pair $(W, W')$.  It is
applied to the situation where $W$ is the sum of the random diagonal of a matrix
to prove the HCCLT.  Chen et al.\ generalize Theorem~\ref{T:stein-theorem} to allow
for situations in which the regression condition does not hold exactly, and
we later in addition relax the assumption that the $W$ has unit variance.
\begin{theorem}[Stein]\label{T:stein-theorem}
  If $W$, $W'$ are mean 0, exchangeable random variables with variance 1
  satisfying the exact regression condition
  \begin{equation*}
    \E[W'-W|W] = -\lambda W
  \end{equation*}
  for some $\lambda \in (0,1)$, then
  \begin{align*}
    \sup_{w \in \mathbb{R}} |P(W \leq w) - \Phi(w)| &\leq
    2\sqrt{\E \left [1 - \frac{1}{2 \lambda}E[(W'-W)^2|W] \right ]} +
    (2 \pi)^{-1/4}\sqrt{\frac{1}{\lambda}\E |W'-W|^3} \\
    &\leq 2 \sqrt{\var (\E [(W'-W)^2|W])} +
    (2 \pi)^{-1/4}\sqrt{\frac{1}{\lambda}\E |W'-W|^3}.
  \end{align*}
\end{theorem}

\section{Hoeffding's Combinatorial CLT}
Stein's proof of the HCCLT relies on an application of Theorem~\ref{T:stein-theorem}.
\begin{theorem}[Hoeffding's Combinatorial Central Limit Theorem]\label{T:HCCLT}
  Let $\{a_{ij}\}_{i,j}$ be an $n \times n$ matrix of real-valued
  entries that is row- and column-centered and scaled such that the sums
  of the squares of its elements equals $n-1$:
  \begin{align*}
    \sum_{j=1}^n a_{ij} &= 0 \\
    \sum_{i=1}^n a_{ij} &= 0 \\
    \sum_{i=1, j=1}^n a_{ij}^2 &= n-1
  \end{align*}
  Let $\Pi$ be a random permutation of $\{1, \ldots, n\}$ drawn
  uniformly at random from the set of all permutations:
  \begin{equation*}
    P(\Pi = \pi) = \frac{1}{n!}.
  \end{equation*}
  Define
  \begin{equation*}
    W = \sum_{i=1}^n a_{i\Pi(i)}
  \end{equation*}
  to be the sum of a random diagonal.  Then
  \begin{equation*}
    |P(W \leq w) - \Phi(w)| \leq
    \frac{C}{\sqrt{n}} \left [
      \sqrt{\sum_{i, j = 1}^{n} a_{ij}^4} +
      \sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}
    \right ].
  \end{equation*}
\end{theorem}

Given fixed data $a_{ij}$, the HCCLT provides a bound in terms of a universal
constant $C$, a sample-size-dependent term $\frac{1}{\sqrt{n}}$,
and a function of the data
$\sqrt{\sum_{i, j = 1}^{n} a_{ij}^4} + \sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}$.
Thus, given $C$, we can calculate an explicit bound on the Kolmogorov distance.
Consider a sequence of matrices, $a_{ij}^{(n)}$, of dimension $n \times n$.
In order to achieve a $\mathcal{O}(n^{-1/2})$ rate of convergence, we require the
function of the data to be bounded.  However, this will not typically be the case.

Bolthausen \cite{bolthausen1984estimate} was able to prove the following result:
\begin{theorem}[Bolthausen]
  Under the conditions of Theorem~\ref{T:HCCLT}, there is an absolute constant
  $K > 0$ such that
  \begin{equation*}
    |P(W \leq w) - \Phi(w)| \leq K \frac{\sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3}}{n}.
  \end{equation*}
\end{theorem}

Then, given the sequence of matrices $a_{ij}^{(n)}$, the theorem
yields a convergence rate of $\mathcal{O}(n^{-1/2})$ as long as
$\sqrt{\sum_{i, j = 1}^{n} |a_{ij}|^3} / \sqrt{n}$ remains bounded.

Now, we return to generalizing Theorem~\ref{T:stein-theorem}.

% \section{Exchangeable Pairs}
% TODO: Add a lot of development for exchangeable pairs.  For now,
% focusing on generalizing the theorems in ``Normal Approximation by
% Stein's Method.'' \cite{chen2010normal}

% Theorem 5.5 in ``Normal Approximation by Stein's Method'' concerns
% variance 1 exchangeable random variables.  Our setting has the
% variance tending to 1, so we first prove a slight generalization of
% the theorem.  Large parts of the proof are copied verbatim from the
% book.
\section{Generalized Stein's Theorems}
Here, we treat the situation where the regression condition fails to hold exactly.
Chen et al.\ \cite{chen2010normal} serves as an excellent reference for results
of this type.

\begin{definition}[Approximate Stein Pair]
  Let $(W, W')$ be an exchangeable pair.  If the pair satisfies the ``approximate linear
  regression condition''
  \begin{equation}
    \label{D:approx-stein-pair}
    \E [W - W' | W] = \lambda (W - R),
  \end{equation}
  where $R$ is a variable of small order and $\lambda \in (0, 1)$, then we call $(W, W')$ an
  approximate Stein pair.
\end{definition}

Here we generalize Lemma 5.1 from \cite{chen2010normal} to the setting of non-unit variance:
\begin{theorem}
  \label{L:stein-difference-second-moment-generalization-bound}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance $\E W^2$
  satisfying
  \begin{equation*}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$, then for any
  $z \in \mathbb{R}$ and $a > 0$,
  \begin{equation*}
    \E [(W'-W)^2 \mathbf{1}_{\{-a \leq W' - W \leq 0\}} \mathbf{1}_{\{z-a \leq W \leq z\}}] \leq
    3a \lambda (\sqrt{\E W^2} + \E |R|)
  \end{equation*}
  and
  \begin{equation*}
    \E [(W'-W)^2 \mathbf{1}_{\{0 \leq W' - W \leq a\}} \mathbf{1}_{\{z-a \leq W \leq z\}}] \leq
        3a \lambda (\sqrt{\E W^2} + \E |R|).
  \end{equation*}
\end{theorem}

The following theorem will allow us to prove a $\mathcal{O}(n^{-1/4})$ rate under
mild conditions.  Generalization of Theorem 5.5 from \cite{chen2010normal}:
\begin{theorem}
  \label{T:main}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance
  $\E W^2$
  satisfying
  \begin{equation*}
    \label{eq:13}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$, then
  \begin{equation*}
    \begin{split}
      \sup_{z \in \mathbb{R}} |P(W \leq z) - \Phi(z)|
      &\leq (2\pi)^{-1/4} \sqrt{\frac{\E |W'-W|^3}{\lambda}}
      + \frac{1}{2\lambda} \sqrt{\var (\E [(W'-W)^2|W])} \\
      &\quad + |\E W^2 - 1| + \E |WR| + \E |R|
    \end{split}
  \end{equation*}
\end{theorem}

The following result will let us achieve a rate of $\mathcal{O}(n^{-1/2})$ subject to an
additional constraint on the data.  Generalization of part of Theorem 5.3 from \cite{chen2010normal}:
\begin{theorem}
  \label{T:better-rate}
  If $W$, $W'$ are mean 0 exchangeable random variables with variance
  $\E W^2$
  satisfying
  \begin{equation*}
    \E[W'-W|W] = -\lambda(W-R)
  \end{equation*}
  for some $\lambda \in (0,1)$ and some random variable $R$ and $|W'-W| \leq \delta$, then
  \begin{equation*}
    \begin{split}
      \sup_{z \in \mathbb{R}} |P(W \leq z) - \Phi(z)|
      &\leq \frac{.41 \delta^3}{\lambda} + 3 \delta (\sqrt{\E W^2} + \E |R|)
      + \frac{1}{2\lambda} \sqrt{\var (\E [(W'-W)^2|W])} \\
      &\quad + |\E W^2 - 1| + \E |WR| + \E |R|
    \end{split}
  \end{equation*}
\end{theorem}
